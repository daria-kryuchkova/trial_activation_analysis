{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b111396-1b75-42ca-a6d2-5cc2e9d21c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import FuncFormatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad55c49-dc84-441a-933f-2a919ce7f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('raw.csv')\n",
    "df_raw.columns = df_raw.columns.str.lower()\n",
    "print(\"shape of the data:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73f1f1-864d-4137-8e8b-bc9d301c5e90",
   "metadata": {},
   "source": [
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e7920-8654-4730-bf95-0b4207e86249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types of all columns\n",
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c458d2-9f18-4829-b36e-8936ed3fb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df, columns, errors='coerce', format=None):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=errors, format=format)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fadde-45ca-4a1c-8bdf-9d1a520262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = convert_to_datetime(df_raw, ['timestamp', 'converted_at', 'trial_start', 'trial_end'])\n",
    "df_raw['converted'] = df_raw['converted'].astype(int)\n",
    "print(df_raw.dtypes)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d46c0-4594-4e0b-9e8f-14d855780711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda36651-2f10-4de0-b912-480c842eeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if 'converted_at' is null is only null when converted=False\n",
    "\n",
    "df_nulls = df_raw[df_raw['converted_at'].isna()]\n",
    "null_summary = df_nulls.groupby('converted').size().reset_index(name='null_count')\n",
    "null_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11477f6-6193-4744-85a8-513cdf6bf2a8",
   "metadata": {},
   "source": [
    "Since organizations that did not convert churned by the end of trial, I will consider their this timestamp as their 'conversion into churn' time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39a2a7-c646-4f10-9fcb-be1dd108ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling converted_at for churned users.\n",
    "\n",
    "#latest_timestamps = df_raw.groupby('organization_id')['timestamp'].transform('max')\n",
    "\n",
    "df_raw['converted_at'] = df_raw.apply(\n",
    "    lambda row: row['trial_end'] if pd.isna(row['converted_at']) and not row['converted'] else row['converted_at'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_raw = df_raw.sort_values(by=['organization_id', 'timestamp']).reset_index(drop=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9125e9-62e5-4dbd-b0bb-10a7a94f7127",
   "metadata": {},
   "source": [
    "For this analysis I need only the activities that happenned before conversion, so I will remove the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7011a1-8033-4321-8bae-74d1b04e87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bc = df[~(df['timestamp'] > df['converted_at'])]\n",
    "#df_bc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51692e-85a4-4cdf-8c4f-1680d59e3291",
   "metadata": {},
   "source": [
    "# 2. Finding and cleaning inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f58e1-a924-4f3b-b547-1792e7d4c44c",
   "metadata": {},
   "source": [
    "- Converted value per organization should remain unchanged\n",
    "- All the timestamps for not converted organizations must be within the trial period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483018d-e921-48c8-9634-d0993c3820d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conversion_consistency(df):\n",
    "    \n",
    "    converted_counts = df.groupby('organization_id')['converted'].nunique()\n",
    "    # Find the organizations that have more than 1 distinct conversion rate\n",
    "    orgs_with_multiple_converted = converted_counts[converted_counts > 1].index.tolist()\n",
    "\n",
    "    if orgs_with_multiple_converted:\n",
    "        return orgs_with_multiple_converted\n",
    "    else:\n",
    "        return \"No inconsistent conversion values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4263af1-a2a1-4c6b-a8a4-9bc70f592bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = check_conversion_consistency(df_raw)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abbe23-c52e-4124-b999-549349009449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_date_consistency(df, return_message=True):\n",
    "\n",
    "    # === Aggregate only necessary columns ===\n",
    "    agg_dict = {\n",
    "        'timestamp': ['min', 'max'],\n",
    "        'converted_at': ['min', 'max'],\n",
    "        'trial_start': ['min', 'max'],\n",
    "        'trial_end': ['min', 'max']\n",
    "    }\n",
    "\n",
    "    date_check = df.groupby('organization_id').agg(agg_dict)\n",
    "    date_check.columns = ['_'.join(col).strip() for col in date_check.columns.values]\n",
    "    date_check = date_check.reset_index()\n",
    "\n",
    "    # === Making sure that converted_at, trial_start and trial_end are unique per onganization ===\n",
    "    for col in ['converted_at', 'trial_start', 'trial_end']:\n",
    "        date_check[f\"{col}_vary_flag\"] = (\n",
    "            date_check[f\"{col}_min\"] != date_check[f\"{col}_max\"]\n",
    "        ).astype(int)\n",
    "\n",
    "    # === Checking that activity timestamps are within allowed ranges ===\n",
    "    date_check['timestamp_min_flag'] = (\n",
    "        date_check['timestamp_min'] <= date_check['trial_start_min']\n",
    "    ).astype(int)\n",
    "\n",
    "    date_check['timestamp_max_flag'] = (\n",
    "        date_check['timestamp_max'] >= date_check['trial_end_max']\n",
    "    ).astype(int)\n",
    "\n",
    "    date_check['converted_at_flag'] = (\n",
    "        date_check['converted_at_min'] < date_check['trial_start_min']\n",
    "    ).astype(int)\n",
    "\n",
    "    # === Identifying suspicious records ===\n",
    "    flag_cols = [col for col in date_check.columns if col.endswith('_flag')]\n",
    "    suspicious = date_check[date_check[flag_cols].sum(axis=1) > 0]\n",
    "\n",
    "    if suspicious.empty:\n",
    "        return \"No inconsistent dates\" if return_message else None\n",
    "\n",
    "    return suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f799c-6769-441f-bcad-53def28b511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = check_date_consistency(df_raw)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90fe60-0cb7-4200-bdd7-3550352e9baf",
   "metadata": {},
   "source": [
    "# 3. Checking for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c3049-01dd-4c01-8cb8-8b5a11b3add7",
   "metadata": {},
   "source": [
    "Checking for duplicated events using a combination of **organization_id**, **activity_name** and **timestamp** as a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754a0d6-5820-4985-aabc-1ef9edf34944",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_records = (\n",
    "    df_raw\n",
    "    .groupby(['organization_id', 'activity_name', 'timestamp'])\n",
    "    .agg(\n",
    "        total_records=('activity_name', 'size')\n",
    "    )\n",
    "    .sort_values(['organization_id','timestamp', 'activity_name'])\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a32af2-f9e2-4baa-836b-c1b42a3a451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orgs_total = df_raw['organization_id'].nunique()\n",
    "num_rows_total = len(df_raw)\n",
    "\n",
    "duplicates = grouped_records[grouped_records['total_records'] > 1]\n",
    "\n",
    "num_orgs_with_duplicates = duplicates['organization_id'].nunique()\n",
    "num_duplicated_rows = duplicates['total_records'].sum()\n",
    "\n",
    "print(\"Total unique organizations:\", num_orgs_total)\n",
    "print(\"Total records:\", num_rows_total)\n",
    "\n",
    "print(\"Number of organizations with duplicates:\", num_orgs_with_duplicates)\n",
    "print(\"Total number of duplicated records:\", num_duplicated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03322c37-e1f7-4b08-9de7-291fa1c1c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of duplicates per activity\n",
    "\n",
    "duplicates_activity = (\n",
    "    duplicates\n",
    "    .groupby('activity_name')\n",
    "    .agg(\n",
    "        total_duplicate_instances=('total_records', 'size'),\n",
    "        total_orgs=('organization_id', 'nunique'),\n",
    "        min_total_records=('total_records', 'min'),\n",
    "        percentile_25=('total_records', lambda x: x.quantile(0.25)),\n",
    "        median_total_records=('total_records', 'median'),\n",
    "        percentile_75=('total_records', lambda x: x.quantile(0.75)),\n",
    "        max_total_records=('total_records', 'max')\n",
    "    )\n",
    "    .sort_values('median_total_records', ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "duplicates_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e9311-5a0c-4b1d-8808-d616de32d238",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Looking at the median total records, I can see that most activities have 2-3 duplicates per organization and timestamp, so that could be a simple coincindece. \n",
    "\n",
    "However, looking at max, there are 3 activities that stand out: \n",
    "\n",
    "- Scheduling.Shift.Created\n",
    "- Scheduling.Shift.AssignmentChanged\n",
    "- Scheduling.Availability.Set\n",
    "\n",
    "I assume that these activities can be triggered in bulk. For example, an Employee can set availability for several days with one click, and an Admin can create several shifts using some presets or a template. \n",
    "\n",
    "I'm going to verify whether they are indeed triggered by something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c23c67-e148-46b9-9f7d-93cd186ab899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying records with max duplicates per activity\n",
    "\n",
    "idx = grouped_records.groupby('activity_name')['total_records'].idxmax()\n",
    "max_per_act = grouped_records.loc[idx]\n",
    "max_per_act = max_per_act.sort_values('total_records', ascending=False).reset_index(drop=True)\n",
    "max_per_act.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9129e7-39b0-4ca0-b3de-cbbabf6d9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that in case of timestamp conflict the template is shown \n",
    "\n",
    "gr_sorted = grouped_records.sort_values(\n",
    "    by=['organization_id', 'timestamp', 'activity_name'],\n",
    "    ascending=[True, True, True]\n",
    ")\n",
    "\n",
    "# Define custom sorting priority\n",
    "activity_priority = {\n",
    "    'Scheduling.Template.ApplyModal.Applied': 0,\n",
    "    'Scheduling.Shift.Created': 1,\n",
    "    'Scheduling.Shift.AssignmentChanged': 2\n",
    "}\n",
    "\n",
    "# Apply priority where applicable\n",
    "gr_sorted['activity_priority'] = gr_sorted['activity_name'].map(activity_priority).fillna(999)\n",
    "\n",
    "# Final sort with priority\n",
    "gr_sorted = gr_sorted.sort_values(\n",
    "    by=['organization_id', 'timestamp', 'activity_priority']\n",
    ").drop(columns='activity_priority').reset_index(drop=True)\n",
    "\n",
    "#gr_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a714a-8051-4369-b9dc-fcfa8552f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_within_interval(\n",
    "    source_df: pd.DataFrame,\n",
    "    index: int,\n",
    "    search_df: pd.DataFrame,\n",
    "    time_before: int = None,\n",
    "    time_after: int = None\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Extract reference values\n",
    "    org_id = source_df.loc[index, 'organization_id']\n",
    "    ref_time = source_df.loc[index, 'timestamp']\n",
    "\n",
    "    # Ensure timestamp column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(search_df['timestamp']):\n",
    "        search_df = search_df.copy()\n",
    "        search_df['timestamp'] = pd.to_datetime(search_df['timestamp'])\n",
    "\n",
    "    # Start with org_id match\n",
    "    mask = (search_df['organization_id'] == org_id)\n",
    "\n",
    "    # Apply optional time bounds\n",
    "    if time_before is not None:\n",
    "        start_time = ref_time - timedelta(minutes=time_before)\n",
    "        mask &= (search_df['timestamp'] >= start_time)\n",
    "\n",
    "    if time_after is not None:\n",
    "        end_time = ref_time + timedelta(minutes=time_after)\n",
    "        mask &= (search_df['timestamp'] <= end_time)\n",
    "\n",
    "    return search_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9e4e5-63db-443a-9532-8a47492d1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records for index 0 of max_record_per_org within 60 minutes\n",
    "result = get_records_within_interval(\n",
    "    source_df=max_per_act,\n",
    "    index=1,\n",
    "    time_before=60,\n",
    "    time_after=60,\n",
    "    search_df=gr_sorted\n",
    ")\n",
    "\n",
    "result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0688104-beb8-4b24-89e3-23479dd58dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_action_ever_used(\n",
    "    source_df: pd.DataFrame,\n",
    "    action_name: str = \"Scheduling.Template.ApplyModal.Applied\",\n",
    "    max_records: int = 10\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = source_df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['organization_id', 'timestamp'])\n",
    "\n",
    "    # Get first time the action was used per organization\n",
    "    action_first_use = (\n",
    "        df[df['activity_name'] == action_name]\n",
    "        .groupby('organization_id')['timestamp']\n",
    "        .min()\n",
    "        .rename('first_template_used')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge with original data\n",
    "    df = df.merge(action_first_use, on='organization_id', how='left')\n",
    "\n",
    "    # template_used_ever = 1 if first_template_used exists and is earlier than current timestamp\n",
    "    df['template_used_ever'] = (\n",
    "        (df['first_template_used'].notna()) &\n",
    "        (df['first_template_used'] <= df['timestamp'])\n",
    "    ).astype(int)\n",
    "\n",
    "    # Drop helper column\n",
    "    df = df.drop(columns='first_template_used')\n",
    "\n",
    "    # Filter if desired\n",
    "    df = df[df['total_records'] >= max_records].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77256f3a-c59b-4f7a-8ae7-f10c2e030f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = annotate_action_ever_used(\n",
    "    source_df=gr_sorted,\n",
    "    action_name=\"Scheduling.Template.ApplyModal.Applied\",\n",
    "    #time_interval=15,     # Optional: only recent activity\n",
    "    max_records=10        # Only annotate rows where total_records >= 10\n",
    ")\n",
    "\n",
    "annotated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96f9cc-3bf0-4a0b-8414-31dfb32543eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where neither recent nor ever action occurred\n",
    "no_template_triggered = annotated_df[\n",
    "    (annotated_df['template_used_ever'] == 0)\n",
    "]\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "#no_template_triggered.head(10)\n",
    "\n",
    "# Find the index of the row with max total_records per organization\n",
    "idx = no_template_triggered.groupby('organization_id')['total_records'].idxmax()\n",
    "max_records_no_template = no_template_triggered.loc[idx].reset_index(drop=True)\n",
    "max_records_no_template.sort_values('total_records', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11f176-58d7-40d3-8f86-a3dbeaaf3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orgs = no_template_triggered['organization_id'].nunique()\n",
    "num_rows = no_template_triggered['total_records'].sum()\n",
    "\n",
    "print(\"Total unique organizations:\", num_orgs)\n",
    "print(\"Total records:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c84ee7-58b1-4e8e-8ba4-3aefe7d5dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows for a specific record\n",
    "check = gr_sorted[\n",
    "    (gr_sorted['organization_id'] == '154647fa6ad39ad1ea4dd6bdfd273679') &\n",
    "    (gr_sorted['timestamp'] >= pd.to_datetime('2024-02-01 00:00:00'))&\n",
    "    (gr_sorted['timestamp'] <= pd.to_datetime('2024-02-22 20:00:00'))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427cf77-c3ab-4a4c-b11e-1cec4f2aec18",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- Scheduling.Shift.Created and Scheduling.Shift.AssignmentChanged- many bulk records are indeed triggered by Scheduling.Template.ApplyModal.Applied\n",
    "- Some duplicated entries are created by organizations that never used the templates\n",
    "- Scheduling.Availability.Set - this is not, so I assume an employee can just submit their availability for the whole year with one click.\n",
    "\n",
    "I will assume that all the duplicated records are valid and keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce2876-ee8c-47d7-b331-61e7dbf6acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condesging the dataset for easier readability\n",
    "activity_priority = {\n",
    "    'Scheduling.Template.ApplyModal.Applied': 0,\n",
    "    'Scheduling.Shift.Created': 1,\n",
    "    'Scheduling.Shift.AssignmentChanged': 2\n",
    "}\n",
    "\n",
    "df_grp = (\n",
    "    df_raw\n",
    "    .groupby(['organization_id', 'activity_name', 'timestamp'], as_index=False)\n",
    "    .agg(\n",
    "        records=('activity_name', 'size'),\n",
    "        converted=('converted', 'first'),\n",
    "        converted_at=('converted_at', 'first'),\n",
    "        trial_start=('trial_start', 'first'),\n",
    "        trial_end=('trial_end', 'first')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_grp['activity_priority'] = df_grp['activity_name'].map(activity_priority).fillna(999)\n",
    "df_grp = df_grp.sort_values(['organization_id', 'timestamp', 'activity_priority']).reset_index(drop=True)\n",
    "df_grp = df_grp.drop(columns='activity_priority')\n",
    "df_grp['bulk'] = (df_grp['records'] > 1).astype(int)\n",
    "\n",
    "df_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c30562-ff75-4ed8-ba14-50a000fee514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_sorted = df_grp.sort_values(['organization_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Creating group_id for consecutive runs of same org/activity_name/bulk\n",
    "df_grp_sorted['group_id'] = (\n",
    "    (df_grp_sorted['organization_id'] != df_grp_sorted['organization_id'].shift()) |\n",
    "    (df_grp_sorted['activity_name'] != df_grp_sorted['activity_name'].shift()) |\n",
    "    (df_grp_sorted['bulk'] != df_grp_sorted['bulk'].shift()) |\n",
    "    (df_grp_sorted['timestamp'].dt.date != df_grp_sorted['timestamp'].dt.date.shift())\n",
    ").cumsum()\n",
    "\n",
    "df_short = (\n",
    "    df_grp_sorted\n",
    "    .groupby('group_id')\n",
    "    .agg(\n",
    "        organization_id=('organization_id', 'first'),\n",
    "        activity_name=('activity_name', 'first'),\n",
    "        bulk=('bulk', 'first'),\n",
    "        ts_start=('timestamp', 'first'),\n",
    "        ts_end=('timestamp', 'last'),\n",
    "        events=('records', 'size'), # number of rows condensed\n",
    "        records=('records', 'sum'), # number of records in rows condensed\n",
    "        converted=('converted', 'first'),\n",
    "        converted_at=('converted_at', 'first'),\n",
    "        trial_start=('trial_start', 'first'),\n",
    "        trial_end=('trial_end', 'first')\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_short['time_diff_sec'] = (df_short['ts_end']-df_short['ts_start']).dt.total_seconds().astype(int)\n",
    "df_short = df_short[['organization_id', 'activity_name', 'ts_start', 'ts_end', 'time_diff_sec', 'bulk', 'events', 'records', 'converted', 'converted_at', 'trial_start','trial_end']]\n",
    "df_short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f432c03-2bdb-4e67-9bc9-b10cc4f34edb",
   "metadata": {},
   "source": [
    "# 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53767062-028b-45f8-9281-152b3a82c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df_short.copy()\n",
    "df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570d673-0ffc-484e-8def-7d0a683e76b0",
   "metadata": {},
   "source": [
    "#### Adding helper columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0a07f-2872-4976-8d34-38f0c01004fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension columns\n",
    "df_eda['first_action_ts'] = df_eda.groupby('organization_id')['ts_start'].transform('min') #first action timestamp\n",
    "df_eda['first_action_weekday'] = df_eda['first_action_ts'].dt.dayofweek\n",
    "df_eda['first_action_hour'] = df_eda['first_action_ts'].dt.hour\n",
    "df_eda['last_action_ts'] = df_eda.groupby('organization_id')['ts_end'].transform('max')\n",
    "df_eda['days_to_action'] = (df_eda['first_action_ts'].dt.date - df_eda['trial_start'].dt.date).dt.days\n",
    "df_eda['active_span_days'] = ((df_eda['last_action_ts'] - df_eda['first_action_ts']).dt.days)+1\n",
    "\n",
    "#Fact columns\n",
    "df_eda['days_since_first_action'] = (df_eda['ts_start'] - df_eda['first_action_ts']).dt.days\n",
    "df_eda['days_since_trial_start'] = (df_eda['ts_start'].dt.date - df_eda['trial_start'].dt.date).dt.days\n",
    "df_eda['days_till_trial_end'] = (df_eda['trial_end'].dt.date - df_eda['ts_end'].dt.date).dt.days\n",
    "\n",
    "df_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e01b6-816c-495f-b10f-fe5e0f79f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do users convert?\n",
    "\n",
    "df_eda['days_to_convert_tr'] = (df_eda['converted_at'].dt.date - df_eda['trial_start'].dt.date).dt.days #since trial start\n",
    "df_eda['days_to_convert_fa'] = (df_eda['converted_at'].dt.date - df_eda['first_action_ts'].dt.date).dt.days #since first action\n",
    "df_eda['days_to_convert_la'] = (df_eda['converted_at'].dt.date - df_eda['last_action_ts'].dt.date).dt.days #since first action\n",
    "\n",
    "#Adding user group based on conversion status an conversion time\n",
    "df_eda['user_group'] = np.select(\n",
    "    [\n",
    "        df_eda['converted'] == 0,  # Not converted\n",
    "        (df_eda['converted'] == 1) & \n",
    "        (df_eda['converted_at'] <= df_eda['trial_end']) & \n",
    "        (df_eda['converted_at'] <= df_eda['last_action_ts']),  # Converted during trial & active after\n",
    "        (df_eda['converted'] == 1) & \n",
    "        (df_eda['converted_at'] <= df_eda['trial_end']) & \n",
    "        (df_eda['converted_at'] > df_eda['last_action_ts']),   # Converted during trial & not active after\n",
    "        (df_eda['converted'] == 1) & \n",
    "        (df_eda['converted_at'] > df_eda['trial_end']),  # Converted after trial\n",
    "    ],\n",
    "    [0, 1, 2, 3],  # Custom group labels\n",
    "    default=4  # Catch-all for invalid/missing cases\n",
    ")\n",
    "\n",
    "#checking if all the organization fall into valud groups (not 3)\n",
    "orgs_in_groups = (\n",
    "    df_eda\n",
    "    .groupby(['user_group'])\n",
    "    .agg(organization_count=('organization_id', 'nunique'))\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "orgs_in_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877d6b9-257e-4d28-99fe-01be49953833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date and week info from ts_start\n",
    "df_eda['activity_date'] = df_eda['ts_start'].dt.date\n",
    "df_eda['activity_week'] = df_eda['ts_start'].dt.to_period('W').astype(str)\n",
    "\n",
    "# Count distinct days per organization\n",
    "active_days_per_org = (\n",
    "    df_eda\n",
    "    .groupby('organization_id')['activity_date']\n",
    "    .nunique()\n",
    "    .rename('active_days')\n",
    ")\n",
    "\n",
    "# Count distinct weeks per organization\n",
    "active_weeks_per_org = (\n",
    "    df_eda\n",
    "    .groupby('organization_id')['activity_week']\n",
    "    .nunique()\n",
    "    .rename('active_weeks')\n",
    ")\n",
    "\n",
    "df_eda = df_eda.merge(active_days_per_org, on='organization_id', how='left')\n",
    "df_eda = df_eda.merge(active_weeks_per_org, on='organization_id', how='left')\n",
    "\n",
    "df_eda.drop(columns=['activity_date', 'activity_week'], inplace=True)\n",
    "\n",
    "df_eda['activity_density'] = df_eda['active_weeks']/df_eda['active_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf096e4-89bc-4094-b78e-eb4f99129c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb004693-aef1-4410-88bb-19711e270488",
   "metadata": {},
   "source": [
    "#### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130dec7-ac6f-4cd1-a99e-28eef662e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing convesion per days to converst since first action\n",
    "\n",
    "converters = (\n",
    "    df_eda[df_eda['converted'] == 1]\n",
    "    .groupby('organization_id')\n",
    "    .agg(days_to_convert=('days_to_convert_tr', 'min')) #change here days_to_convert_tr to days_to_convert_fa to check that parameter\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Group by days_to_convert\n",
    "time_to_convert = (\n",
    "    converters\n",
    "    .groupby('days_to_convert')\n",
    "    .agg(total_orgs=('organization_id', 'nunique'))\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add cumulative converters and % share\n",
    "time_to_convert['cumulative_converters'] = time_to_convert['total_orgs'].cumsum()\n",
    "total_converted = time_to_convert['total_orgs'].sum()\n",
    "time_to_convert['cumulative_percent'] = (time_to_convert['cumulative_converters'] / total_converted * 100).round(1)\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add absolute converters (left y-axis)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=time_to_convert['days_to_convert'],\n",
    "        y=time_to_convert['total_orgs'],\n",
    "        name='Daily Converters',\n",
    "        marker_color='steelblue',\n",
    "        yaxis='y1'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cumulative % converters (right y-axis)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=time_to_convert['days_to_convert'],\n",
    "        y=time_to_convert['cumulative_percent'],\n",
    "        name='Cumulative %',\n",
    "        mode='lines+markers',\n",
    "        marker_color='orange',\n",
    "        yaxis='y2'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout with dual axes\n",
    "fig.update_layout(\n",
    "    title='Daily and Cumulative Conversion by Days Since Trial Start',\n",
    "    xaxis=dict(title='Trial Day', dtick=1),\n",
    "    yaxis=dict(\n",
    "        title='Number of Converted Orgs',\n",
    "        side='left',\n",
    "        showgrid=False\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Cumulative % of Conversions',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        range=[0, 100]\n",
    "    ),\n",
    "    legend=dict(x=0.01, y=0.99),\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3b1ab-3acf-4f71-a77e-050c586d0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda['trial_start_week'] = df_eda['trial_start'].dt.to_period('W').apply(lambda r: r.start_time).dt.normalize()\n",
    "df_eda['conversion_week'] = df_eda['converted_at'].dt.to_period('W').apply(lambda r: r.start_time).dt.normalize()\n",
    "\n",
    "df_eda['weeks_since_trial_start'] = ((df_eda['conversion_week'] - df_eda['trial_start_week']).dt.days // 7).clip(lower=0)\n",
    "\n",
    "converted = df_eda[df_eda['converted'] == 1].copy()\n",
    "\n",
    "conversion_counts = (\n",
    "    converted.groupby(['trial_start_week', 'weeks_since_trial_start'])\n",
    "    .agg(converted_orgs=('organization_id', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cohort_sizes = (\n",
    "    df_eda.groupby('trial_start_week')\n",
    "    .agg(total_orgs=('organization_id', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "conversion_counts = conversion_counts.merge(cohort_sizes, on='trial_start_week')\n",
    "conversion_counts['conversion_rate'] = conversion_counts['converted_orgs'] / conversion_counts['total_orgs']\n",
    "\n",
    "heatmap_data = conversion_counts.pivot(index='trial_start_week', columns='weeks_since_trial_start', values='conversion_rate')\n",
    "\n",
    "heatmap_data = heatmap_data.sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax = sns.heatmap(heatmap_data, annot=True, fmt=\".1%\", cmap=\"Blues\", cbar_kws={'label': 'Conversion Rate'})\n",
    "\n",
    "ax.set_yticklabels([d.strftime('%Y-%m-%d') for d in heatmap_data.index], rotation=0)\n",
    "\n",
    "plt.title('Weekly Cohort Conversion Rate Heatmap')\n",
    "plt.ylabel('Trial Start Week')\n",
    "plt.xlabel('Weeks Since Trial Start')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b0c21-89bc-444f-86ef-5b17f49eea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[['ts_start', 'trial_start', 'trial_end', 'converted_at']].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f46ad1-6f85-4046-b5c7-3e15dd624f38",
   "metadata": {},
   "source": [
    "### Observtions: \n",
    "- All converted organizations bought the subscriotion at least 2 weeks after trial start and first action\n",
    "- Most of them did that within the last week of trial\n",
    "- Around a half of converters did it after the trial end (30 days). Of those, most converted by day 45 since trial start.\n",
    "- Some weeks were better than the others in terms of overall performance\n",
    "\n",
    "#### Questions: \n",
    "- Does this mean that these organizations were consistently active during the first 2 weeks of trial?\n",
    "- Was this conversion pattern consistent across install cohorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770c706-8c89-4c35-887c-5a5e20846006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter converters\n",
    "df_converted = df_eda[df_eda['converted'] == 1].copy()\n",
    "\n",
    "# Calculate week numbers relative to trial start\n",
    "df_converted['week_last_action'] = ((df_converted['last_action_ts'] - df_converted['trial_start']).dt.days // 7)+1\n",
    "df_converted['weeks_to_convert_tr'] = (df_converted['days_to_convert_tr']// 7)+1\n",
    "\n",
    "\n",
    "# Aggregate conversion info + last active trial week\n",
    "converters = (\n",
    "    df_converted\n",
    "    .groupby('organization_id')\n",
    "    .agg(\n",
    "        weeks_to_convert=('weeks_to_convert_tr', 'min'),\n",
    "        last_active_trial_week=('week_last_action', 'max')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Heatmap: days_to_convert vs last_active_trial_week\n",
    "heatmap = (\n",
    "    converters\n",
    "    .groupby(['weeks_to_convert', 'last_active_trial_week'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "heatmap_norm = heatmap / heatmap.values.sum()\n",
    "annotations = heatmap_norm.applymap(lambda x: f\"{x:.1%}\" if x > 0 else \"\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_norm,\n",
    "    cmap='viridis',\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    annot=annotations,\n",
    "    fmt=\"\",\n",
    "    annot_kws={\"size\": 8}, \n",
    "    cbar_kws = {\n",
    "    'label': 'Share of Total Converters',\n",
    "    'format': mticker.PercentFormatter(xmax=1.0)\n",
    "    }\n",
    ")\n",
    "plt.title('Weeks to Convert vs Last Active Trial Week')\n",
    "plt.xlabel('Last Active Trial Week')\n",
    "plt.ylabel('Weeks to Convert from Trial Start')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2eb82-c23b-4d51-b371-7d891ee04b42",
   "metadata": {},
   "source": [
    "#### Observtions & Questions: \n",
    "- Absolute majority of converters is only active on week 1 (0-6 days since trial start), but convert around the time of trial end.\n",
    "- What is weekly retention like for convertes and non-converters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d7a51-4155-44a4-8754-bef1bab30625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df_viz = df_eda.copy()\n",
    "\n",
    "# Step 1: DAILY retention\n",
    "daily_counts = (\n",
    "    df_viz\n",
    "    .groupby(['days_since_first_action', 'converted'])['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='active_orgs')\n",
    ")\n",
    "\n",
    "daily_counts['retention'] = daily_counts.groupby('converted')['active_orgs'].transform(lambda x: x / x.max())\n",
    "daily_counts['interval'] = 'Daily'\n",
    "daily_counts = daily_counts.rename(columns={'days_since_first_action': 'period'})\n",
    "\n",
    "# Step 2: WEEKLY retention\n",
    "df_viz['weeks_since_first_action'] = (df_viz['days_since_first_action'] // 7).astype(int)\n",
    "\n",
    "weekly_counts = (\n",
    "    df_viz\n",
    "    .groupby(['weeks_since_first_action', 'converted'])['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='active_orgs')\n",
    ")\n",
    "\n",
    "weekly_counts['retention'] = weekly_counts.groupby('converted')['active_orgs'].transform(lambda x: x / x.max())\n",
    "weekly_counts['interval'] = 'Weekly'\n",
    "weekly_counts = weekly_counts.rename(columns={'weeks_since_first_action': 'period'})\n",
    "\n",
    "# Combine both for easy plotting\n",
    "combined = pd.concat([daily_counts, weekly_counts], ignore_index=True)\n",
    "\n",
    "# Plot side-by-side using Plotly subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Daily Retention\", \"Weekly Retention\"),\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Add traces for each interval\n",
    "for converted_val in combined['converted'].unique():\n",
    "    # Daily traces\n",
    "    daily_data = combined[(combined['converted'] == converted_val) & (combined['interval'] == 'Daily')]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=daily_data['period'],\n",
    "            y=daily_data['retention'],\n",
    "            mode='lines+markers',\n",
    "            name=f'Daily - Converted {converted_val}',\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Weekly traces\n",
    "    weekly_data = combined[(combined['converted'] == converted_val) & (combined['interval'] == 'Weekly')]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=weekly_data['period'],\n",
    "            y=weekly_data['retention'],\n",
    "            mode='lines+markers',\n",
    "            name=f'Weekly - Converted {converted_val}',\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Daily vs Weekly Retention by Conversion Status\",\n",
    "    height=400,\n",
    "    width=1000,\n",
    "    legend_title=\"Group\",\n",
    "    xaxis_title=\"Days Since First Action\",\n",
    "    xaxis2_title=\"Weeks Since First Action\",\n",
    "    yaxis_title=\"Normalized Retention\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986804ae-60a0-497e-a3fa-4f4f2a02d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = df_eda.copy()\n",
    "df_viz = df_viz[df_eda['active_span_days'] > 1]\n",
    "\n",
    "org_activity = (\n",
    "    df_viz\n",
    "    .groupby(['organization_id', 'converted'])\n",
    "    .agg(\n",
    "        active_days=('active_days', 'max'),\n",
    "        active_weeks=('active_weeks', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "org_activity['days_per_week'] = org_activity['active_days'] / org_activity['active_weeks']\n",
    "\n",
    "fig = px.box(\n",
    "    org_activity,\n",
    "    x='converted',\n",
    "    y='days_per_week',\n",
    "    title='Distribution of Active Days per Active Week by Conversion Status',\n",
    "    labels={'converted': 'Converted', 'days_per_week': 'Active Weeks / Active Days'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1462d-5f64-4955-a4c3-230e705daf31",
   "metadata": {},
   "source": [
    "#### Observtions: \n",
    "- Daily retention: 80% of both converters and non-converters churn after day1\n",
    "- After Day 1 both daily and weekly retention is stable, starting from week 2 Converted orgs have slightly higher retention\n",
    "- Converted organizations have higher weekly engagement density (2.45 vs 2 days) and lower weekly engagement variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0398516-44ef-4006-87b7-2a93d40b7c28",
   "metadata": {},
   "source": [
    "#### Observations & Questions\n",
    "- Converted organizations with activity span between 15 and 28 days had fewer total active days on average compared to not converters (i.e. lower engagement density)\n",
    "- Oterwise, there is no noticeable difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3e1c8-7a66-401a-933e-c66e8cc7f0c7",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Converted organizations with activity span between 15 and 28 days had fewer total active days on average compared to not converters (i.e. lower engagement density)\n",
    "- Oterwise, there is no noticeable difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308c4ee-f0a8-4861-b6f2-57503fe01902",
   "metadata": {},
   "source": [
    "### Engagement rates per activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35240394-cdfa-4589-808e-4359e5a868aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all the conversions happen 14+ days since first action, I will use the first 2 weeks as a prediction dataset to normalise the data\n",
    "\n",
    "#df_cap = df_eda[df_eda['days_since_first_action'] < 14].reset_index(drop=True)\n",
    "df_cap = df_eda[df_eda['ts_start'] <= df_eda['converted_at']].reset_index(drop=True)\n",
    "df_cap.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102be55-7f39-4c56-b255-84393945b4ca",
   "metadata": {},
   "source": [
    "#### Identifying activities with the highest engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae753e-c1cd-4aec-838c-a1b8195ad6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_engagement_by_category(\n",
    "    df,\n",
    "    category_col,                # e.g. 'activity_name' or 'employee_act'\n",
    "    user_group_col='user_group',\n",
    "    org_col='organization_id',\n",
    "    time_col='ts_start'\n",
    "):\n",
    "    # Total records and orgs engaged\n",
    "    summary_table = (\n",
    "        df\n",
    "        .groupby([category_col, user_group_col])\n",
    "        .agg(\n",
    "            total_records=(time_col, 'count'),\n",
    "            orgs_engaged=(org_col, 'nunique')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot tables\n",
    "    pivot_total_records = summary_table.pivot(\n",
    "        index=category_col,\n",
    "        columns=user_group_col,\n",
    "        values='total_records'\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    pivot_orgs_engaged = summary_table.pivot(\n",
    "        index=category_col,\n",
    "        columns=user_group_col,\n",
    "        values='orgs_engaged'\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    merged_pivot = pd.concat(\n",
    "        [pivot_total_records, pivot_orgs_engaged],\n",
    "        axis=1,\n",
    "        keys=['total_records', 'orgs_engaged']\n",
    "    )\n",
    "\n",
    "    # Engagement rate\n",
    "    \n",
    "    total_orgs_per_group = df.groupby(user_group_col)[org_col].nunique().to_dict()\n",
    "\n",
    "    engagement_rate = merged_pivot['orgs_engaged'].copy()\n",
    "    for col in engagement_rate.columns:\n",
    "        engagement_rate[col] = engagement_rate[col] / total_orgs_per_group.get(col, 1)\n",
    "\n",
    "    engagement_rate.columns = pd.MultiIndex.from_product(\n",
    "        [['engagement_rate'], engagement_rate.columns]\n",
    "    )\n",
    "\n",
    "    # Merge everything\n",
    "    merged_with_engagement = pd.concat([merged_pivot, engagement_rate], axis=1)\n",
    "\n",
    "    # Add \"overall\" columns for sorting\n",
    "    merged_with_engagement[('total_records', 'overall')] = merged_with_engagement['total_records'].sum(axis=1)\n",
    "    merged_with_engagement[('orgs_engaged', 'overall')] = merged_with_engagement['orgs_engaged'].sum(axis=1)\n",
    "    merged_with_engagement[('engagement_rate', 'overall')] = (\n",
    "        merged_with_engagement[('orgs_engaged', 'overall')] /\n",
    "        sum(total_orgs_per_group.values())\n",
    "    )\n",
    "\n",
    "    merged_with_engagement = merged_with_engagement.sort_values(\n",
    "        by=('engagement_rate', 'overall'),\n",
    "        ascending=False\n",
    "    )\n",
    "\n",
    "    merged_with_engagement = merged_with_engagement.drop(\n",
    "        columns=[\n",
    "            ('total_records', 'overall'),\n",
    "            ('orgs_engaged', 'overall'),\n",
    "            ('engagement_rate', 'overall')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return merged_with_engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc105b2-29ba-4ff6-9a5c-8de945638e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the treshold for the most relevant activities\n",
    "\n",
    "engagement = summarize_engagement_by_category(df_cap, category_col='activity_name')\n",
    "\n",
    "# Filtering the events with at least 5% engagement rate in any user group\n",
    "top_engagement = engagement[\n",
    "    (engagement[('engagement_rate', 0)] > 0.02) |\n",
    "    (engagement[('engagement_rate', 1)] > 0.02) |\n",
    "    (engagement[('engagement_rate', 2)] > 0.02) \n",
    "]\n",
    "\n",
    "styled_engagement = (\n",
    "    top_engagement\n",
    "    .reset_index()\n",
    "    .style\n",
    "    .background_gradient(subset=['engagement_rate'], cmap='viridis')\n",
    "    .format({\n",
    "        ('engagement_rate', 0): '{:.1%}',\n",
    "        ('engagement_rate', 1): '{:.1%}',\n",
    "        ('engagement_rate', 2): '{:.1%}'\n",
    "    })\n",
    "    .set_caption(\"Engagement rate per activity type\")\n",
    ")\n",
    "styled_engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9c94a-ca35-4f9a-8c5e-ec85668fd76c",
   "metadata": {},
   "source": [
    "#### Observations & Questions:\n",
    "\n",
    "-  _Scheduling.Shift.Created_  engagement is around 90% for both converters and non-converters. Why do organizations convert if they never scheduled a shift?\n",
    "- _Mobile.Schedule.Loaded_  is used by only about 50% of organizations. Does this mean they don't have employees? Or they use it in admin-only mode?\n",
    "- _PunchClock.PunchedOut_\tis not in top 10 by engagement and not even close to  _PunchClock.PunchedIn_ - do they punch out automatically?\n",
    "- _Scheduling.Shift.AssignmentChanged_, _PunchClock.PunchedIn_ and _Scheduling.Shift.Approved_ activities are very different for Group 1 (converters during trial) and could be good predictors.\n",
    "- _Scheduling.Template.ApplyModal.Applied_ is much higher for Group 1 (converters after trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8c3dc-5684-424f-ab7c-3e4779bb9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engagement_summary_no_activity(df, exclude_activities=None, mode='or'):\n",
    "    \"\"\"\n",
    "    Generate an engagement summary for organizations that did NOT use specified activity/activities.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Source dataframe with required columns\n",
    "    - exclude_activities (str or list of str): Activities to exclude by name\n",
    "    - mode (str): 'or' (default) excludes orgs with any of the activities;\n",
    "                  'and' excludes only those with all the activities\n",
    "\n",
    "    Returns:\n",
    "    - pd.Styler: Styled engagement summary\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize exclude_activities input\n",
    "    if exclude_activities is None:\n",
    "        exclude_activities = []\n",
    "    elif isinstance(exclude_activities, str):\n",
    "        exclude_activities = [exclude_activities]\n",
    "\n",
    "    # Find organizations to exclude\n",
    "    if mode == 'or':\n",
    "        # Exclude orgs that used ANY of the activities\n",
    "        orgs_with_excluded = df[df['activity_name'].isin(exclude_activities)]['organization_id'].unique()\n",
    "\n",
    "    elif mode == 'and':\n",
    "        # Find orgs that used ALL of the activities\n",
    "        activity_per_org = df[df['activity_name'].isin(exclude_activities)].groupby('organization_id')['activity_name'].nunique()\n",
    "        orgs_with_excluded = activity_per_org[activity_per_org == len(set(exclude_activities))].index\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be either 'or' or 'and'\")\n",
    "\n",
    "    # Filter out excluded orgs\n",
    "    df_filtered = df[~df['organization_id'].isin(orgs_with_excluded)].sort_values(['organization_id', 'ts_start']).reset_index(drop=True)\n",
    "\n",
    "    # Count remaining organizations\n",
    "    org_count = df_filtered['organization_id'].nunique()\n",
    "\n",
    "    # Summarize engagement\n",
    "    engagement = summarize_engagement_by_category(df_filtered, category_col='activity_name')\n",
    "\n",
    "    excluded_label = ', '.join(exclude_activities) if exclude_activities else \"None\"\n",
    "    logic_label = \"any of\" if mode == 'or' else \"all of\"\n",
    "\n",
    "    # Styled result\n",
    "    styled = (\n",
    "        engagement\n",
    "        .reset_index()\n",
    "        .style\n",
    "        .background_gradient(subset=['engagement_rate'], cmap='viridis')\n",
    "        .format({\n",
    "            ('engagement_rate', 0): '{:.1%}',\n",
    "            ('engagement_rate', 1): '{:.1%}',\n",
    "            ('engagement_rate', 2): '{:.1%}'\n",
    "        })\n",
    "        .set_caption(\n",
    "            f\"<b><span style='font-size:16px'>Engagement \"\n",
    "            f\"excluding orgs that used {logic_label}:</b> <br/>{excluded_label} . <br/> <b> Total: {org_count} organizations</span></b>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fd747-4309-4342-bb44-1edbb1171b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_shifts = engagement_summary_no_activity(df_cap, exclude_activities=['A', 'B'], mode='or')\n",
    "\n",
    "no_shifts = engagement_summary_no_activity(df_cap, exclude_activities=['Scheduling.Shift.Created'], mode='or')\n",
    "no_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4aa3f-6006-40e7-a8c3-788a59b4009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of activities to exclude\n",
    "#employee_activities = ['Mobile.Schedule.Loaded', 'PunchClock.PunchedIn', 'Absence.Request.Created', 'Scheduling.Availability.Set', 'Scheduling.OpenShiftRequest.Created']\n",
    "employee_activities = ['Mobile.Schedule.Loaded']\n",
    "\n",
    "# Then pass it to your function\n",
    "no_employee = engagement_summary_no_activity(df_cap, exclude_activities=employee_activities, mode='or')\n",
    "no_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28e7b3-b802-4aa3-9e30-4334cbc9267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shifts_mobile = engagement_summary_no_activity(df_cap, exclude_activities=['Scheduling.Shift.Created', 'Mobile.Schedule.Loaded'], mode='or')\n",
    "no_shifts_mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb86168-e378-4515-b9cd-ac566c7a7aff",
   "metadata": {},
   "source": [
    "#### Observations and assumptions: \n",
    "- The organizations that didn't engage in sheduling shifts, mainly engaged in viewing schedule on mobile and didn't engage in any meaningful activity involving scheduling, time tracking or accounting.\n",
    "- Around 50% of total organizations (both converted and not) did not engage in any employee-driven activities. I assume that it is possible to use Planday in admin-only mode without explicit time tracking.\n",
    "- Mobile.Schedule.Loaded has the biggest overlap with other emploee activities, so I will use it as a predictor for any employee activity.\n",
    "- Almost all the organizations (except 14) engaged either in Scheduling.Shift.Created or Mobile.Schedule.Loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7857c2-20c0-4b51-a155-d74c5616718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Makrov chains to see which events happened right before churn \n",
    "\n",
    "df_chains = df_cap.sort_values(['organization_id', 'ts_start'])\n",
    "\n",
    "# Adding 'churned' and 'converted' states\n",
    "\n",
    "final_activities = df_cap[['organization_id', 'converted', 'converted_at']].drop_duplicates(subset=['organization_id'])\n",
    "final_activities['activity_name'] = final_activities['converted'].apply(lambda x: 'Converted' if x == 1 else 'Churned')\n",
    "final_activities['ts_start'] = final_activities['converted_at']\n",
    "final_activities = final_activities[['organization_id', 'activity_name', 'ts_start']]\n",
    "\n",
    "df_chains_all = pd.concat([df_chains[['organization_id', 'activity_name', 'ts_start']], final_activities], ignore_index=True)\n",
    "df_chains_sorted = df_chains_all.sort_values(['organization_id', 'ts_start'])\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_transitions(df, title):\n",
    "    df['next_state'] = df.groupby('organization_id')['activity_name'].shift(-1)\n",
    "    \n",
    "    transitions = (\n",
    "        df.dropna(subset=['next_state'])\n",
    "        .groupby(['activity_name', 'next_state'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    \n",
    "    transition_matrix = transitions.pivot(index='activity_name', columns='next_state', values='count').fillna(0)\n",
    "    transition_prob = transition_matrix.div(transition_matrix.sum(axis=1), axis=0)\n",
    "    \n",
    "    cols_to_plot = [col for col in ['Converted', 'Churned'] if col in transition_prob.columns]\n",
    "    if not cols_to_plot:\n",
    "        print(\"No transitions to 'Converted' or 'Churned' found.\")\n",
    "        return\n",
    "    \n",
    "    final_transitions = transition_prob[cols_to_plot].copy()\n",
    "    \n",
    "    # Scale each column independently to [0,1]\n",
    "    final_transitions_scaled = final_transitions.apply(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0\n",
    "    )\n",
    "    \n",
    "    # Sort by sum of scaled values for order\n",
    "    final_transitions_scaled['sum'] = final_transitions_scaled.sum(axis=1)\n",
    "    final_transitions_scaled = final_transitions_scaled.sort_values('sum', ascending=False)\n",
    "    \n",
    "    # Select top 10 activities only\n",
    "    final_transitions_scaled_top10 = final_transitions_scaled.head(10).drop(columns='sum')\n",
    "    \n",
    "    # Melt for plotly express\n",
    "    plot_df = final_transitions_scaled_top10.reset_index().melt(\n",
    "        id_vars='activity_name', value_vars=cols_to_plot,\n",
    "        var_name='Transition To', value_name='Scaled Probability'\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "    plot_df,\n",
    "    y='activity_name',\n",
    "    x='Scaled Probability',\n",
    "    color='Transition To',\n",
    "    orientation='h',\n",
    "    barmode='group',\n",
    "    title=title,\n",
    "    labels={'activity_name': 'Current State', 'Scaled Probability': 'Scaled Transition Probability'},\n",
    "    color_discrete_map={\n",
    "        'Converted': 'orange',\n",
    "        'Churned': 'steelblue'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "    fig.show() \n",
    "    \n",
    "plot_transitions(df_chains_sorted, \"Scaled Transitions to 'Converted' and 'Churned'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821913c0-78f4-4998-8c24-117ef2763ffc",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "Some activities have \n",
    "- Scheduling.Shift.Created and Mobile.Schedule.Loaded  - these are core functions of the app and they have the highest engagement rate, so they alone are unlikely to cause conversion or churn\n",
    "- Scheduling.Shift.Approved, Scheduling.Shift.AssignementChanged and Absence.Request.Approved have the highest difference between converted and churned groups\n",
    "- Timesheets.BulkApprove.Confirmed only leads to churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b67a9f-b5db-4eb3-bf5d-228af4200d39",
   "metadata": {},
   "source": [
    "## Feature engineering and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3541f6-0c77-4157-89f7-f15bc3895ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 100)\n",
    "df_fe = df_cap.copy()\n",
    "df_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd4bab-350f-41ab-b83e-cd37f3eab66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim = (\n",
    "    df_fe\n",
    "    .groupby(['organization_id', 'converted'])\n",
    "    .agg(\n",
    "        days_to_action =('days_to_action', 'first'),\n",
    "        first_action_weekday=('first_action_weekday', 'first'),\n",
    "        first_action_hour=('first_action_hour', 'first'),\n",
    "        active_span_days =('active_span_days', 'first'),\n",
    "        active_days =('active_days', 'first'),\n",
    "        active_weeks =('active_weeks', 'first'),\n",
    "        activity_density =('activity_density', 'first')\n",
    "    )\n",
    "    .sort_values(['organization_id', 'converted'])\n",
    "    .reset_index()\n",
    ")\n",
    "#df_dim['first_action_week'] = df_fe['first_action_ts'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "d1_flag = df_fe[df_fe['days_since_first_action'] == 1] \\\n",
    "            .groupby('organization_id').size() \\\n",
    "            .to_frame('d1_eng')\n",
    "\n",
    "d1_flag['d1_eng'] = 1\n",
    "df_dim = df_dim.merge(d1_flag[['d1_eng']], on='organization_id', how='left')\n",
    "df_dim['d1_eng'] = df_dim['d1_eng'].fillna(0).astype(int)\n",
    "\n",
    "df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5d13f-310e-43f0-a079-aff804d11337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_custom_metrics(df_eda, day_start=0, day_end=9999):\n",
    "    # Step 1: Filter by days\n",
    "    df_filtered = df_eda[\n",
    "        (df_eda['days_since_first_action'] >= day_start) &\n",
    "        (df_eda['days_since_first_action'] <= day_end)\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    all_orgs = df_filtered['organization_id'].unique()\n",
    "\n",
    "    # Get max days per org to normalise the counts\n",
    "    max_days = df_filtered.groupby('organization_id')['days_since_first_action'].max().replace(0, 1)\n",
    "\n",
    "    # Top activities with at least 3% engagement\n",
    "    activities = [\n",
    "    'Scheduling.Shift.Created',\n",
    "    'Mobile.Schedule.Loaded',\n",
    "    'Scheduling.Shift.AssignmentChanged',\n",
    "    #'PunchClock.PunchedIn',\n",
    "    #'PunchClock.PunchedOut',\n",
    "    'Scheduling.Shift.Approved',\n",
    "    'Communication.Message.Created',\n",
    "    'Scheduling.Template.ApplyModal.Applied',\n",
    "    'Scheduling.Availability.Set',\n",
    "    'Absence.Request.Created',\n",
    "    'Absence.Request.Approved',\n",
    "    'Timesheets.BulkApprove.Confirmed'\n",
    "    ]\n",
    "\n",
    "    agg = df_filtered.groupby(['organization_id', 'activity_name', 'bulk'], dropna=False).agg(\n",
    "        total_records=('records', 'sum'),\n",
    "        total_events=('events', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Step 6: Pivot for easier metric access\n",
    "    records_pivot = agg.pivot_table(index=['organization_id', 'activity_name'], columns='bulk',\n",
    "                                    values='total_records', fill_value=0)\n",
    "    events_pivot = agg.pivot_table(index=['organization_id', 'activity_name'], columns='bulk',\n",
    "                                   values='total_events', fill_value=0)\n",
    "\n",
    "    # Step 7: Total events per org per activity\n",
    "    total_events_per_activity = agg.groupby(['organization_id', 'activity_name'])['total_events'].sum().unstack(fill_value=0)\n",
    "\n",
    "    # Step 8: Prepare index of orgs\n",
    "    orgs = pd.Index(all_orgs, name='organization_id')\n",
    "\n",
    "    # Step 9: Helpers\n",
    "    def get_records(activity, bulk=None):\n",
    "        try:\n",
    "            if bulk is None:\n",
    "                return records_pivot.xs(activity, level='activity_name').sum(axis=1).reindex(orgs, fill_value=0)\n",
    "            return records_pivot.loc[(slice(None), activity), bulk].droplevel('activity_name').reindex(orgs, fill_value=0)\n",
    "        except KeyError:\n",
    "            return pd.Series(0, index=orgs)\n",
    "\n",
    "    def get_events(activity, bulk=None):\n",
    "        try:\n",
    "            if bulk is None:\n",
    "                return events_pivot.xs(activity, level='activity_name').sum(axis=1).reindex(orgs, fill_value=0)\n",
    "            return events_pivot.loc[(slice(None), activity), bulk].droplevel('activity_name').reindex(orgs, fill_value=0)\n",
    "        except KeyError:\n",
    "            return pd.Series(0, index=orgs)\n",
    "\n",
    "    # Step 10: Compute metrics\n",
    "    df_metrics = pd.DataFrame(index=orgs)\n",
    "\n",
    "    df_metrics['template_usage'] = get_events('Scheduling.Template.ApplyModal.Applied') / get_events('Scheduling.Shift.Created')\n",
    "    df_metrics['availability_to_shifts'] = get_events('Scheduling.Availability.Set') / get_events('Scheduling.Shift.Created')\n",
    "    df_metrics['views_to_shifts'] = get_records('Mobile.Schedule.Loaded') / get_records('Scheduling.Shift.Created')\n",
    "    df_metrics['punch_in_views'] = get_records('PunchClock.PunchedIn') / get_records('Mobile.Schedule.Loaded')\n",
    "    #df_metrics['punch_in_out'] = get_records('PunchClock.PunchedOut') / get_records('PunchClock.PunchedIn')\n",
    "    df_metrics['change_rate'] = get_records('Scheduling.Shift.AssignmentChanged') / get_records('Scheduling.Shift.Created')\n",
    "    df_metrics['messages_shifts'] = get_events('Communication.Message.Created') / get_events('Scheduling.Shift.Created')\n",
    "    df_metrics['messages_views'] = get_records('Scheduling.Shift.AssignmentChanged') / get_records('Mobile.Schedule.Loaded')\n",
    "    df_metrics['approve_rate'] = get_records('Scheduling.Shift.Approved') / get_records('Scheduling.Shift.Created')\n",
    "\n",
    "    # Step 11: Add normalized per-day total events\n",
    "    for activity in activities:\n",
    "        col_name = f'daily_{activity}'\n",
    "        if activity in total_events_per_activity.columns:\n",
    "            normalized = total_events_per_activity[activity].reindex(orgs, fill_value=0) / max_days.reindex(orgs, fill_value=1)\n",
    "        else:\n",
    "            normalized = pd.Series(0, index=orgs)\n",
    "        df_metrics[col_name] = normalized\n",
    "    \n",
    "    # Sum records for each activity across all bulk types\n",
    "    activity_records_sum = records_pivot.groupby(level=['organization_id', 'activity_name']).sum()\n",
    "\n",
    "    # Step 12: Cleanup\n",
    "    df_metrics.replace([float('inf'), -float('inf')], 0, inplace=True)\n",
    "    df_metrics.fillna(0, inplace=True)\n",
    "\n",
    "    return df_metrics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d09fb-053d-4347-b3f6-65ae14c6fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on just 2 days of engagement\n",
    "metrics_df = calculate_custom_metrics(df_fe, day_start=0, day_end=14)\n",
    "\n",
    "'''\n",
    "cols_to_drop = [\n",
    "    col for col in metrics_df.select_dtypes(include='number').columns\n",
    "    if not col.startswith('daily_') and col != 'converted'\n",
    "]\n",
    "metrics_df = metrics_df.drop(columns=cols_to_drop)\n",
    "'''\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9746da-cbc1-4fe7-8acb-4242e8323df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_var_cols = metrics_df.columns[metrics_df.nunique() <= 1]\n",
    "print(\"Columns with zero or one unique value:\", zero_var_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c17c94-f076-4955-9b0c-cc4cf293bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_dim.merge(metrics_df, on='organization_id', how='left')\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40beb6-2d1d-4316-9f51-c02c29ea5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_feat.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2160aa6-a0af-4020-b002-6c6276aacf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_feat.head())\n",
    "print(df_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918a8a5-34af-4f84-8a8d-3978ff2d717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn imbalanced-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7ee6e-d8ee-41e1-bc36-2bc52038ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "df=df_feat.copy()\n",
    "\n",
    "X = df.drop(columns=['converted', 'organization_id'])\n",
    "y = df['converted']\n",
    "\n",
    "#categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "#X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)  \n",
    "\n",
    "# Now you can continue with scaling, modeling, etc.\n",
    "scaler = StandardScaler()\n",
    "#X_scaled = pd.DataFrame(scaler.fit_transform(X_encoded), columns=X_encoded.columns)\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train, predict, and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = None\n",
    "    # For AUROC, need predicted probabilities for the positive class\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):  # fallback\n",
    "        y_proba = model.decision_function(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    # AUROC\n",
    "    if y_proba is not None:\n",
    "        auroc = roc_auc_score(y_test, y_proba)\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "    else:\n",
    "        print(\"AUROC: N/A (no probability estimates available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f58512-6f49-48c0-8bfc-e1b962e84598",
   "metadata": {},
   "source": [
    "#### Prediction result\n",
    "\n",
    "XGBoost model has shown the best accuracy overall. Although more feature engineering and pruning to improve Converters prediction, we can look at the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab6061-ffb3-4aa9-a373-e103610c8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Fit your model (already done)\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Create SHAP explainer for tree-based models\n",
    "explainer = shap.Explainer(xgb_model, X_train_res)\n",
    "\n",
    "# Calculate SHAP values for the training set\n",
    "shap_values = explainer(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10516899-3aa0-40a2-aacd-bfb34913502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_res, max_display=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633be0b9-8cf1-4918-ac45-a3133fb49cd7",
   "metadata": {},
   "source": [
    "#### Features\n",
    "According to the model, the most important features. Here are the ones that had a clear positive or negative contribution:\n",
    "\n",
    "Independent metrics:\n",
    "\n",
    "- _Days to action_: organizations that start using the app on the day of trial start are more likely to convert\n",
    "\n",
    "Note: daily metrics were capped (only first 3 days), the above are historic\n",
    "\n",
    "App metrics:\n",
    "\n",
    "- Organizations with high _Shift assignement change rate_ are less likely to convert\n",
    "- Organizations with higher _Punch in to View_ ratio are more likely to convert\n",
    "- Organizations with higher _Template usage_ are more likely to convert\n",
    "- Organizations with low ratio of _Views to Shifts_ scheduled are more likely to convert -> this could indicate that actively run a business and already have real employees\n",
    "- Organizations with high number of _Messages Created_ scheduled are less likely to convert -> this could indicate that the messaging tool is not convenient, or that users don't understaand how to use other fun\n",
    "\n",
    "Punch in to punch out rate is quite low and often 0, so I assume that this is an optional action and employees are punched out automatically when their shift ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ef781-dc25-4b6c-9cad-99467327f5ad",
   "metadata": {},
   "source": [
    "### Trial goals\n",
    "\n",
    "Based on the model and EDA results, I am going to monitor these trial goals:\n",
    "\n",
    "Core functionality:\n",
    "Scheduling.Shift.Created -> Mobile.Schedule.Loaded -> Scheduling.Shift.Approved\n",
    "\n",
    "Admin functionality:\n",
    "Scheduling.Template.ApplyModal.Applied -> Scheduling.Shift.Created ->  Scheduling.Shift.AssignmentChanged -> Scheduling.Shift.Approved\n",
    "\n",
    "Since there is no hard sequence of events, I ordered them in the order or the most intuitive discovery in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c8741-ed18-4375-b0e7-4d26556c9c53",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f82d-8779-4825-a32a-f6d884c8a2b6",
   "metadata": {},
   "source": [
    "### 2.1 Trial goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d831b-bc3b-49d7-af55-45a78652636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the same table as made in SQL\n",
    "# I will take df_raw as the source\n",
    "\n",
    "funnel_steps = {\n",
    "    'Scheduling.Shift.Created': 1,\n",
    "    'Mobile.Schedule.Loaded': 2,\n",
    "    'Scheduling.Shift.AssignmentChanged': 3,\n",
    "    'Scheduling.Shift.Approved': 4,\n",
    "    'Scheduling.Template.ApplyModal.Applied': 5\n",
    "}\n",
    "\n",
    "organizations =  (\n",
    "    df_raw\n",
    "    .groupby(['organization_id'])\n",
    "    .agg(\n",
    "        trial_start=('trial_start', 'min'),\n",
    "        trial_start_real=('timestamp', 'min'),\n",
    "        trial_end=('trial_end', 'max')\n",
    "    )\n",
    "    .sort_values(['organization_id'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "organizations['trial_week'] = organizations['trial_start'].dt.to_period('W').apply(lambda r: r.start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744be19-eb6d-4ca5-b0dc-3bb4715198bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_goals = df_raw[df_raw['activity_name'].isin(funnel_steps.keys())].copy()\n",
    "\n",
    "# Map funnel steps\n",
    "trial_goals['funnel_step'] = trial_goals['activity_name'].map(funnel_steps)\n",
    "\n",
    "# Keep only required columns\n",
    "trial_goals = trial_goals[[\n",
    "    'timestamp',\n",
    "    'organization_id',\n",
    "    'funnel_step',\n",
    "    'activity_name'\n",
    "]]\n",
    "\n",
    "trial_goals_mart=trial_goals.merge(organizations, on='organization_id', how='inner')\n",
    "trial_goals_mart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f5bf8-b44c-4e90-82c3-5a4f3bfa0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_counts = (\n",
    "    trial_goals_mart\n",
    "    .groupby(['activity_name', 'funnel_step'])['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='orgs')\n",
    ")\n",
    "\n",
    "funnel_counts = funnel_counts.sort_values('funnel_step')\n",
    "\n",
    "first_step_count = funnel_counts.loc[funnel_counts['funnel_step'] == 1, 'orgs'].values[0]\n",
    "funnel_counts['share'] = funnel_counts['orgs'] / first_step_count\n",
    "funnel_counts['label'] = funnel_counts['orgs'].astype(str) + ' orgs (' + (funnel_counts['share'] * 100).round(1).astype(str) + '%)'\n",
    "\n",
    "fig = px.funnel(\n",
    "    funnel_counts,\n",
    "    y='activity_name',\n",
    "    x='orgs',\n",
    "    labels={'orgs': 'Organizations', 'activity_name': 'Funnel Step'},\n",
    "    title='Trial Goals Completed by Organizations'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        categoryorder='array',\n",
    "        categoryarray=funnel_counts.sort_values('funnel_step')['activity_name'].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "for i, row in funnel_counts.iterrows():\n",
    "    fig.add_annotation(\n",
    "        x=row['orgs'],\n",
    "        y=row['activity_name'],\n",
    "        text=row['label'],\n",
    "        showarrow=False,\n",
    "        xanchor='left',\n",
    "        yanchor='middle',\n",
    "        font=dict(size=12, color='black')\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa9404-dd69-4e0c-93aa-f5db84bbe853",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Completion rate of the least popular trial goal is 12,7%, but since they are not strictly locked on one another, total completion rate could be lower.\n",
    "Also, this is all data for all the trial cohorts, so some organizations didn't have enough time to complete them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1568d3-2a7f-454b-a6f7-522f461c9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "\n",
    "funnel_heatmap_data = (\n",
    "    trial_goals_mart\n",
    "    .groupby(['trial_week', 'activity_name'])['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='orgs_completed')\n",
    ")\n",
    "\n",
    "weekly_orgs = (\n",
    "    organizations\n",
    "    .groupby('trial_week')['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='total_orgs')\n",
    ")\n",
    "\n",
    "funnel_heatmap_data = funnel_heatmap_data.merge(weekly_orgs, on='trial_week')\n",
    "funnel_heatmap_data['completion_rate'] = funnel_heatmap_data['orgs_completed'] / funnel_heatmap_data['total_orgs']\n",
    "\n",
    "fig = px.imshow(\n",
    "    funnel_heatmap_data.pivot(index='activity_name', columns='trial_week', values='completion_rate'),\n",
    "    labels=dict(x=\"Trial Start Week\", y=\"Funnel Step (Activity)\", color=\"Completion Rate\"),\n",
    "    color_continuous_scale='Blues',\n",
    "    title=\"Funnel Completion Rate by Trial Start Week\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Trial Start Week\",\n",
    "    yaxis_title=\"Activity\",\n",
    "    yaxis=dict(\n",
    "        categoryorder='array',\n",
    "        categoryarray=list(funnel_steps.keys())  # Optional: enforce funnel order\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae13275-e062-46ff-a6b5-1c62c04a0d74",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Trial goal copmletion per weekly cohort varies from 8% to 21% depending on trial start date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bfa1bd-c1f0-4acb-8b1d-10d28dd89107",
   "metadata": {},
   "source": [
    "### 2.2 Trial activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062f65a-6d96-4a7c-9cf9-4878870025fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial activation summary per organization\n",
    "trial_activation = (\n",
    "    trial_goals_mart\n",
    "    .groupby('organization_id')\n",
    "    .agg(\n",
    "        max_step_ts=('timestamp', 'max'),\n",
    "        max_funnel_step=('funnel_step', 'max'),\n",
    "        total_steps=('activity_name', 'nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Determine trial_status\n",
    "trial_activation['trial_status'] = trial_activation['total_steps'].apply(\n",
    "    lambda x: 'Completed' if x > 4 else 'Not Completed'\n",
    ")\n",
    "trial_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be32543-4618-45f2-a559-b6b918607621",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_activation_mart = trial_activation.merge(\n",
    "    organizations,\n",
    "    on='organization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "trial_activation_mart['days_since_trial_start'] = (\n",
    "    (trial_activation_mart['max_step_ts'] - trial_activation_mart['trial_start']).dt.days\n",
    ")\n",
    "trial_activation_mart['days_since_trial_start_real'] = (\n",
    "    (trial_activation_mart['max_step_ts'] - trial_activation_mart['trial_start_real']).dt.days\n",
    ")\n",
    "\n",
    "funnel_step_reverse = {v: k for k, v in funnel_steps.items()}\n",
    "trial_activation_mart['last_trial_activity'] = trial_activation_mart['max_funnel_step'].map(funnel_step_reverse)\n",
    "\n",
    "trial_activation_mart.rename(columns={\n",
    "    'max_step_ts': 'updated_at',\n",
    "    'max_funnel_step': 'last_trial_step'\n",
    "}, inplace=True)\n",
    "\n",
    "trial_activation_mart = trial_activation_mart[[\n",
    "    'organization_id',\n",
    "    'updated_at',\n",
    "    'days_since_trial_start_real',\n",
    "    'days_since_trial_start',\n",
    "    'last_trial_step',\n",
    "    'last_trial_activity',\n",
    "    'total_steps',\n",
    "    'trial_status',\n",
    "    'trial_start',\n",
    "    'trial_start_real',\n",
    "    'trial_end',\n",
    "    'trial_week'\n",
    "]]\n",
    "\n",
    "trial_activation_mart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf555c-6a7b-4e65-a52c-e916c7089e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_heatmap = (\n",
    "    trial_activation_mart\n",
    "    .groupby(['trial_week', 'trial_status'])\n",
    "    .size()\n",
    "    .reset_index(name='org_count')\n",
    ")\n",
    "\n",
    "total_per_week = (\n",
    "    trial_activation_mart\n",
    "    .groupby('trial_week')['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='total_orgs')\n",
    ")\n",
    "\n",
    "status_heatmap = status_heatmap.merge(total_per_week, on='trial_week')\n",
    "status_heatmap['percent'] = (status_heatmap['org_count'] / status_heatmap['total_orgs']) * 100\n",
    "status_heatmap['label'] = status_heatmap.apply(\n",
    "    lambda row: f\"{row['percent']:.1f}%\", axis=1\n",
    ")\n",
    "\n",
    "z_values = status_heatmap.pivot(index='trial_status', columns='trial_week', values='org_count').fillna(0)\n",
    "text_labels = status_heatmap.pivot(index='trial_status', columns='trial_week', values='label').fillna(\"\")\n",
    "\n",
    "ordered_status = ['Completed', 'Not Completed', 'Unknown']\n",
    "z_values = z_values.reindex([s for s in ordered_status if s in z_values.index])\n",
    "text_labels = text_labels.reindex(z_values.index)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=z_values.values,\n",
    "    x=z_values.columns.astype(str),\n",
    "    y=z_values.index,\n",
    "    text=text_labels.values,\n",
    "    texttemplate=\"%{text}\",\n",
    "    colorscale='Blues',\n",
    "    colorbar_title=\"Organizations\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Trial Status Distribution by Trial Week\",\n",
    "    xaxis_title=\"Trial Week\",\n",
    "    yaxis_title=\"Trial Status\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7d874-9cb9-4c8c-acd4-e0de9b0230cd",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Overall trial activation rate based on completing **all** trial goals is between 2.4% and 10% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7243a5-de58-49f3-b49c-2227729ffc5c",
   "metadata": {},
   "source": [
    "## Saving mart_ tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71408ba2-245d-4a68-8cde-1d40ad09d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_goals_mart.to_csv('trial_goals_mart.csv', index=False)\n",
    "trial_activation_mart.to_csv('trial_activation_mart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665f784-e440-4582-8b8e-d6a4e35f470b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
