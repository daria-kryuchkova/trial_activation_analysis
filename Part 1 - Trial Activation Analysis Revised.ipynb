{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b111396-1b75-42ca-a6d2-5cc2e9d21c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad55c49-dc84-441a-933f-2a919ce7f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('raw.csv')\n",
    "df_raw.columns = df_raw.columns.str.lower()\n",
    "print(\"shape of the data:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73f1f1-864d-4137-8e8b-bc9d301c5e90",
   "metadata": {},
   "source": [
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e7920-8654-4730-bf95-0b4207e86249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types of all columns\n",
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c458d2-9f18-4829-b36e-8936ed3fb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df, columns, errors='coerce', format=None):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=errors, format=format)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fadde-45ca-4a1c-8bdf-9d1a520262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = convert_to_datetime(df_raw, ['timestamp', 'converted_at', 'trial_start', 'trial_end'])\n",
    "df_raw['converted'] = df_raw['converted'].astype(int)\n",
    "print(df_raw.dtypes)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d46c0-4594-4e0b-9e8f-14d855780711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda36651-2f10-4de0-b912-480c842eeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if 'converted_at' is null is only null when converted=False\n",
    "\n",
    "df_nulls = df_raw[df_raw['converted_at'].isna()]\n",
    "null_summary = df_nulls.groupby('converted').size().reset_index(name='null_count')\n",
    "null_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11477f6-6193-4744-85a8-513cdf6bf2a8",
   "metadata": {},
   "source": [
    "Since organizations that did not convert churned by the end of trial, I will consider trial_end timestamp as their 'conversion into churn' time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39a2a7-c646-4f10-9fcb-be1dd108ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling converted_at for churned users.\n",
    "\n",
    "#latest_timestamps = df_raw.groupby('organization_id')['timestamp'].transform('max')\n",
    "\n",
    "df_raw['converted_at'] = df_raw.apply(\n",
    "    lambda row: row['trial_end'] if pd.isna(row['converted_at']) and not row['converted'] else row['converted_at'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_raw = df_raw.sort_values(by=['organization_id', 'timestamp']).reset_index(drop=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51692e-85a4-4cdf-8c4f-1680d59e3291",
   "metadata": {},
   "source": [
    "# 2. Finding and cleaning inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f58e1-a924-4f3b-b547-1792e7d4c44c",
   "metadata": {},
   "source": [
    "- Converted value per organization should remain unchanged\n",
    "- All the timestamps for not converted organizations must be within the trial period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483018d-e921-48c8-9634-d0993c3820d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conversion_consistency(df):\n",
    "    \n",
    "    converted_counts = df.groupby('organization_id')['converted'].nunique()\n",
    "    # Find the organizations that have more than 1 distinct conversion rate\n",
    "    orgs_with_multiple_converted = converted_counts[converted_counts > 1].index.tolist()\n",
    "\n",
    "    if orgs_with_multiple_converted:\n",
    "        return orgs_with_multiple_converted\n",
    "    else:\n",
    "        return \"No inconsistent conversion values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4263af1-a2a1-4c6b-a8a4-9bc70f592bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = check_conversion_consistency(df_raw)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abbe23-c52e-4124-b999-549349009449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_date_consistency(df, return_message=True):\n",
    "\n",
    "    # === Aggregate only necessary columns ===\n",
    "    agg_dict = {\n",
    "        'timestamp': ['min', 'max'],\n",
    "        'converted_at': ['min', 'max'],\n",
    "        'trial_start': ['min', 'max'],\n",
    "        'trial_end': ['min', 'max']\n",
    "    }\n",
    "\n",
    "    date_check = df.groupby('organization_id').agg(agg_dict)\n",
    "    date_check.columns = ['_'.join(col).strip() for col in date_check.columns.values]\n",
    "    date_check = date_check.reset_index()\n",
    "\n",
    "    # === Making sure that converted_at, trial_start and trial_end are unique per onganization ===\n",
    "    for col in ['converted_at', 'trial_start', 'trial_end']:\n",
    "        date_check[f\"{col}_vary_flag\"] = (\n",
    "            date_check[f\"{col}_min\"] != date_check[f\"{col}_max\"]\n",
    "        ).astype(int)\n",
    "\n",
    "    # === Checking that activity timestamps are within allowed ranges ===\n",
    "    date_check['timestamp_min_flag'] = (\n",
    "        date_check['timestamp_min'] <= date_check['trial_start_min']\n",
    "    ).astype(int)\n",
    "\n",
    "    date_check['timestamp_max_flag'] = (\n",
    "        date_check['timestamp_max'] >= date_check['trial_end_max']\n",
    "    ).astype(int)\n",
    "\n",
    "    date_check['converted_at_flag'] = (\n",
    "        date_check['converted_at_min'] < date_check['trial_start_min']\n",
    "    ).astype(int)\n",
    "\n",
    "    # === Identifying suspicious records ===\n",
    "    flag_cols = [col for col in date_check.columns if col.endswith('_flag')]\n",
    "    suspicious = date_check[date_check[flag_cols].sum(axis=1) > 0]\n",
    "\n",
    "    if suspicious.empty:\n",
    "        return \"No inconsistent dates\" if return_message else None\n",
    "\n",
    "    return suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f799c-6769-441f-bcad-53def28b511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = check_date_consistency(df_raw)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90fe60-0cb7-4200-bdd7-3550352e9baf",
   "metadata": {},
   "source": [
    "# 3. Checking for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c3049-01dd-4c01-8cb8-8b5a11b3add7",
   "metadata": {},
   "source": [
    "Checking for duplicated events using a combination of **organization_id**, **activity_name** and **timestamp** as a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754a0d6-5820-4985-aabc-1ef9edf34944",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_records = (\n",
    "    df_raw\n",
    "    .groupby(['organization_id', 'activity_name', 'timestamp'])\n",
    "    .agg(\n",
    "        total_records=('activity_name', 'size')\n",
    "    )\n",
    "    .sort_values(['organization_id','timestamp', 'activity_name'])\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a32af2-f9e2-4baa-836b-c1b42a3a451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orgs_total = df_raw['organization_id'].nunique()\n",
    "num_rows_total = len(df_raw)\n",
    "\n",
    "duplicates = grouped_records[grouped_records['total_records'] > 1]\n",
    "\n",
    "num_orgs_with_duplicates = duplicates['organization_id'].nunique()\n",
    "num_duplicated_rows = duplicates['total_records'].sum()\n",
    "\n",
    "print(\"Total unique organizations:\", num_orgs_total)\n",
    "print(\"Total records:\", num_rows_total)\n",
    "\n",
    "print(\"Number of organizations with duplicates:\", num_orgs_with_duplicates)\n",
    "print(\"Total number of duplicated records:\", num_duplicated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03322c37-e1f7-4b08-9de7-291fa1c1c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of duplicates per activity\n",
    "\n",
    "duplicates_activity = (\n",
    "    duplicates\n",
    "    .groupby('activity_name')\n",
    "    .agg(\n",
    "        total_duplicate_instances=('total_records', 'size'),\n",
    "        total_orgs=('organization_id', 'nunique'),\n",
    "        min_total_records=('total_records', 'min'),\n",
    "        percentile_25=('total_records', lambda x: x.quantile(0.25)),\n",
    "        median_total_records=('total_records', 'median'),\n",
    "        percentile_75=('total_records', lambda x: x.quantile(0.75)),\n",
    "        percentile_90=('total_records', lambda x: x.quantile(0.90)),\n",
    "        max_total_records=('total_records', 'max')\n",
    "    )\n",
    "    .sort_values('max_total_records', ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "duplicates_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e9311-5a0c-4b1d-8808-d616de32d238",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "Looking at the median total records, I can see that most activities have 2-3 duplicates per organization and timestamp, so that could be a simple coincindece. \n",
    "\n",
    "However, looking at max, there are 3 activities that stand out: \n",
    "\n",
    "- Scheduling.Shift.Created\n",
    "- Scheduling.Shift.AssignmentChanged\n",
    "- Scheduling.Availability.Set\n",
    "\n",
    "I assume that these activities can be triggered in bulk. For example, an Employee can set availability for several days with one click, and an Admin can create several shifts using some presets or a template. \n",
    "\n",
    "I'm going to verify whether they are indeed triggered by something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c23c67-e148-46b9-9f7d-93cd186ab899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying records with max duplicates per activity\n",
    "\n",
    "idx = grouped_records.groupby('activity_name')['total_records'].idxmax()\n",
    "max_per_act = grouped_records.loc[idx]\n",
    "max_per_act = max_per_act.sort_values('total_records', ascending=False).reset_index(drop=True)\n",
    "max_per_act.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9129e7-39b0-4ca0-b3de-cbbabf6d9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that in case of timestamp conflict the template is shown \n",
    "\n",
    "gr_sorted = grouped_records.sort_values(\n",
    "    by=['organization_id', 'timestamp', 'activity_name'],\n",
    "    ascending=[True, True, True]\n",
    ")\n",
    "\n",
    "# Define custom sorting priority\n",
    "activity_priority = {\n",
    "    'Scheduling.Template.ApplyModal.Applied': 0,\n",
    "    'Scheduling.Shift.Created': 1,\n",
    "    'Scheduling.Shift.AssignmentChanged': 2\n",
    "}\n",
    "\n",
    "# Apply priority where applicable\n",
    "gr_sorted['activity_priority'] = gr_sorted['activity_name'].map(activity_priority).fillna(999)\n",
    "\n",
    "# Final sort with priority\n",
    "gr_sorted = gr_sorted.sort_values(\n",
    "    by=['organization_id', 'timestamp', 'activity_priority']\n",
    ").drop(columns='activity_priority').reset_index(drop=True)\n",
    "\n",
    "#gr_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a714a-8051-4369-b9dc-fcfa8552f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_within_interval(\n",
    "    source_df: pd.DataFrame,\n",
    "    index: int,\n",
    "    search_df: pd.DataFrame,\n",
    "    time_before: int = None,\n",
    "    time_after: int = None\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Extract reference values\n",
    "    org_id = source_df.loc[index, 'organization_id']\n",
    "    ref_time = source_df.loc[index, 'timestamp']\n",
    "\n",
    "    # Ensure timestamp column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(search_df['timestamp']):\n",
    "        search_df = search_df.copy()\n",
    "        search_df['timestamp'] = pd.to_datetime(search_df['timestamp'])\n",
    "\n",
    "    # Start with org_id match\n",
    "    mask = (search_df['organization_id'] == org_id)\n",
    "\n",
    "    # Apply optional time bounds\n",
    "    if time_before is not None:\n",
    "        start_time = ref_time - timedelta(minutes=time_before)\n",
    "        mask &= (search_df['timestamp'] >= start_time)\n",
    "\n",
    "    if time_after is not None:\n",
    "        end_time = ref_time + timedelta(minutes=time_after)\n",
    "        mask &= (search_df['timestamp'] <= end_time)\n",
    "\n",
    "    return search_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9e4e5-63db-443a-9532-8a47492d1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records for index 0 of max_record_per_org within 60 minutes\n",
    "result = get_records_within_interval(\n",
    "    source_df=max_per_act,\n",
    "    index=1,\n",
    "    time_before=60,\n",
    "    time_after=60,\n",
    "    search_df=gr_sorted\n",
    ")\n",
    "\n",
    "result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0688104-beb8-4b24-89e3-23479dd58dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_action_ever_used(\n",
    "    source_df: pd.DataFrame,\n",
    "    action_name: str = \"Scheduling.Template.ApplyModal.Applied\",\n",
    "    max_records: int = 10\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = source_df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['organization_id', 'timestamp'])\n",
    "\n",
    "    # Get first time the action was used per organization\n",
    "    action_first_use = (\n",
    "        df[df['activity_name'] == action_name]\n",
    "        .groupby('organization_id')['timestamp']\n",
    "        .min()\n",
    "        .rename('first_template_used')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge with original data\n",
    "    df = df.merge(action_first_use, on='organization_id', how='left')\n",
    "\n",
    "    # template_used_ever = 1 if first_template_used exists and is earlier than current timestamp\n",
    "    df['template_used_ever'] = (\n",
    "        (df['first_template_used'].notna()) &\n",
    "        (df['first_template_used'] <= df['timestamp'])\n",
    "    ).astype(int)\n",
    "\n",
    "    # Drop helper column\n",
    "    df = df.drop(columns='first_template_used')\n",
    "\n",
    "    # Filter if desired\n",
    "    df = df[df['total_records'] >= max_records].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77256f3a-c59b-4f7a-8ae7-f10c2e030f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = annotate_action_ever_used(\n",
    "    source_df=gr_sorted,\n",
    "    action_name=\"Scheduling.Template.ApplyModal.Applied\",\n",
    "    max_records=50        # Only annotate rows where total_records >= x\n",
    ")\n",
    "\n",
    "annotated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96f9cc-3bf0-4a0b-8414-31dfb32543eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where neither recent nor ever action occurred\n",
    "no_template_triggered = annotated_df[\n",
    "    (annotated_df['template_used_ever'] == 0)\n",
    "]\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "#no_template_triggered.head(10)\n",
    "\n",
    "# Find the index of the row with max total_records per organization\n",
    "idx = no_template_triggered.groupby('organization_id')['total_records'].idxmax()\n",
    "max_records_no_template = no_template_triggered.loc[idx].reset_index(drop=True)\n",
    "max_records_no_template.sort_values('total_records', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11f176-58d7-40d3-8f86-a3dbeaaf3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_orgs = no_template_triggered['organization_id'].nunique()\n",
    "num_rows = no_template_triggered['total_records'].sum()\n",
    "max = no_template_triggered['total_records'].sum()\n",
    "\n",
    "print(\"Total unique organizations:\", num_orgs)\n",
    "print(\"Total records:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c84ee7-58b1-4e8e-8ba4-3aefe7d5dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows for a specific record\n",
    "check = gr_sorted[\n",
    "    (gr_sorted['organization_id'] == '154647fa6ad39ad1ea4dd6bdfd273679') &\n",
    "    (gr_sorted['timestamp'] >= pd.to_datetime('2024-02-10 00:00:00'))&\n",
    "    (gr_sorted['timestamp'] <= pd.to_datetime('2024-02-22 20:00:00'))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427cf77-c3ab-4a4c-b11e-1cec4f2aec18",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- Scheduling.Shift.Created and Scheduling.Shift.AssignmentChanged- many bulk records are indeed triggered by Scheduling.Template.ApplyModal.Applied\n",
    "- Some duplicated entries are created by organizations that never used the templates\n",
    "- Duplicate entries with no templates applied have no more than 86 shifts booken in the same second\n",
    "- Scheduling.Availability.Set - this is not, so I assume an employee can just submit their availability for the whole year with one click.\n",
    "\n",
    "I will assume that all the duplicated records are valid and keep them, but mark them as '.Bulk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce2876-ee8c-47d7-b331-61e7dbf6acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condesging the dataset for easier readability\n",
    "\n",
    "activity_priority = {\n",
    "    'Scheduling.Template.ApplyModal.Applied': 0,\n",
    "    'Scheduling.Shift.Created': 1,\n",
    "    'Scheduling.Shift.AssignmentChanged': 2,\n",
    "    'Scheduling.Availability.Set': 3\n",
    "}\n",
    "\n",
    "df_grp = (\n",
    "    df_raw\n",
    "    .groupby(['organization_id', 'activity_name', 'timestamp'], as_index=False)\n",
    "    .agg(\n",
    "        records=('activity_name', 'size'),\n",
    "        converted=('converted', 'first'),\n",
    "        converted_at=('converted_at', 'first'),\n",
    "        trial_start=('trial_start', 'first'),\n",
    "        trial_end=('trial_end', 'first')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_grp['activity_priority'] = df_grp['activity_name'].map(activity_priority).fillna(999)\n",
    "df_grp = df_grp.sort_values(['organization_id', 'timestamp', 'activity_priority']).reset_index(drop=True)\n",
    "df_grp = df_grp.drop(columns='activity_priority')\n",
    "\n",
    "# Will use 3 records as a treshold for bulk records\n",
    "df_grp['bulk'] = (df_grp['records'] > 3).astype(int)\n",
    "\n",
    "df_grp['activity_name_ext'] = np.where(\n",
    "    df_grp['bulk'] == 1,\n",
    "    df_grp['activity_name'] + '.Bulk',\n",
    "    df_grp['activity_name']\n",
    ")\n",
    "\n",
    "df_grp = df_grp.drop(columns='bulk')\n",
    "\n",
    "df_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c30562-ff75-4ed8-ba14-50a000fee514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_sorted = df_grp.sort_values(['organization_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Creating group_id for consecutive runs of same org/activity_name/bulk\n",
    "df_grp_sorted['group_id'] = (\n",
    "    (df_grp_sorted['organization_id'] != df_grp_sorted['organization_id'].shift()) |\n",
    "    (df_grp_sorted['activity_name_ext'] != df_grp_sorted['activity_name'].shift()) |\n",
    "    (df_grp_sorted['timestamp'].dt.date != df_grp_sorted['timestamp'].dt.date.shift())\n",
    ").cumsum()\n",
    "\n",
    "df_short = (\n",
    "    df_grp_sorted\n",
    "    .groupby('group_id')\n",
    "    .agg(\n",
    "        organization_id=('organization_id', 'first'),\n",
    "        activity_name=('activity_name', 'first'),\n",
    "        activity_name_ext=('activity_name_ext', 'first'),\n",
    "        ts_start=('timestamp', 'first'),\n",
    "        ts_end=('timestamp', 'last'),\n",
    "        events=('records', 'size'), # number of rows condensed\n",
    "        records=('records', 'sum'), # number of records in rows condensed\n",
    "        converted=('converted', 'first'),\n",
    "        converted_at=('converted_at', 'first'),\n",
    "        trial_start=('trial_start', 'first'),\n",
    "        trial_end=('trial_end', 'first')\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_short['time_diff_sec'] = (df_short['ts_end']-df_short['ts_start']).dt.total_seconds().astype(int)\n",
    "df_short['activity_name_new'] = df_short['activity_name']\n",
    "df_short = df_short[['organization_id', 'activity_name', 'activity_name_ext', 'ts_start', 'ts_end', 'time_diff_sec', 'events', 'records', 'converted', 'converted_at', 'trial_start','trial_end']]\n",
    "df_short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f432c03-2bdb-4e67-9bc9-b10cc4f34edb",
   "metadata": {},
   "source": [
    "# 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53767062-028b-45f8-9281-152b3a82c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df_short.copy()\n",
    "df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570d673-0ffc-484e-8def-7d0a683e76b0",
   "metadata": {},
   "source": [
    "#### Adding helper columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fc0fb-870c-4bd4-95c9-ef71f14de55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop intermediate columns if they exist (safe for reruns)\n",
    "    for col in ['activity_date', 'activity_week', 'active_days', 'active_weeks', 'max_active_week', 'activity_density']:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=col, inplace=True)\n",
    "\n",
    "    # Dimension columns\n",
    "    df['first_action_ts'] = df.groupby('organization_id')['ts_start'].transform('min')\n",
    "    df['first_action_weekday'] = df['first_action_ts'].dt.dayofweek\n",
    "    df['first_action_hour'] = df['first_action_ts'].dt.hour\n",
    "    df['last_action_ts'] = df.groupby('organization_id')['ts_end'].transform('max')\n",
    "    df['days_to_action'] = (df['first_action_ts'].dt.date - df['trial_start'].dt.date).dt.days\n",
    "    df['active_span_days'] = (df['last_action_ts'] - df['first_action_ts']).dt.days + 1\n",
    "\n",
    "    # Fact columns\n",
    "    df['hours_since_first_action'] = np.floor((df['ts_start'] - df['first_action_ts']).dt.total_seconds() / 3600).astype(int)\n",
    "    df['hours_since_trial_start'] = np.floor((df['ts_start'] - df['trial_start']).dt.total_seconds() / 3600).astype(int)\n",
    "    \n",
    "    df['days_since_first_action'] = (df['ts_start'].dt.date - df['first_action_ts'].dt.date).apply(lambda x: x.days)\n",
    "    df['days_since_trial_start'] = (df['ts_start'].dt.date - df['trial_start'].dt.date).apply(lambda x: x.days)\n",
    "\n",
    "    df['days_to_convert_tr'] = (df['converted_at'].dt.date - df['trial_start'].dt.date).apply(lambda x: x.days)\n",
    "    df['days_to_convert_fa'] = (df['converted_at'].dt.date - df['first_action_ts'].dt.date).apply(lambda x: x.days)\n",
    "    df['days_to_convert_la'] = (df['converted_at'].dt.date - df['last_action_ts'].dt.date).apply(lambda x: x.days)\n",
    "\n",
    "\n",
    "    # Weekly conversion info\n",
    "    df['trial_start_week'] = df['trial_start'].dt.to_period('W').apply(lambda r: r.start_time).dt.normalize()\n",
    "    df['converted_week'] = ((df['converted_at'].dt.date - df['trial_start'].dt.date).apply(lambda x: x.days) // 7).clip(lower=0) + 1\n",
    "\n",
    "    # Extract activity date and week\n",
    "    df['activity_date'] = df['ts_start'].dt.date\n",
    "    df['activity_week'] = ((df['ts_start'].dt.date - df['trial_start'].dt.date).apply(lambda x: x.days) // 7).clip(lower=0)+ 1\n",
    "\n",
    "    # Count distinct active days per org\n",
    "    active_days_per_org = (\n",
    "        df.groupby('organization_id')['activity_date']\n",
    "        .nunique()\n",
    "        .rename('active_days')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Count distinct active weeks and max active week per org\n",
    "    active_weeks_per_org = (\n",
    "        df.groupby('organization_id')\n",
    "        .agg(\n",
    "            active_weeks=('activity_week', 'nunique'),\n",
    "            max_active_week=('activity_week', 'max')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge back into df\n",
    "    df = df.merge(active_days_per_org, on='organization_id', how='left')\n",
    "    df = df.merge(active_weeks_per_org, on='organization_id', how='left')\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(columns=['activity_date', 'activity_week'], inplace=True)\n",
    "\n",
    "    # Final metrics\n",
    "    df['activity_density'] = df['active_weeks'] / df['active_days']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c8966-2ff7-4700-8d7b-30231cc03976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding hepler columns\n",
    "df_eda = add_time_features(df_eda)\n",
    "df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f8697-f62d-4f72-8f20-13ccd6e28819",
   "metadata": {},
   "source": [
    "#### Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e01b6-816c-495f-b10f-fe5e0f79f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do users convert?\n",
    "\n",
    "#Adding user group based on conversion status an conversion time\n",
    "df_eda['user_group'] = np.select(\n",
    "    [\n",
    "        df_eda['converted'] == 0,  # Not converted\n",
    "        (df_eda['converted'] == 1) & \n",
    "        (df_eda['converted_at'] < df_eda['trial_end'])&\n",
    "        (df_eda['last_action_ts'] > df_eda['converted_at']),  # Converted during trial, active later\n",
    "        (df_eda['converted'] == 1) & \n",
    "        (df_eda['converted_at'] < df_eda['trial_end'])&\n",
    "        (df_eda['last_action_ts'] <= df_eda['converted_at']),  # Converted during trial, inactive later\n",
    "        (df_eda['converted'] == 1) & \n",
    "        (df_eda['converted_at'] >= df_eda['trial_end']),  # Converted after trial\n",
    "    ],\n",
    "    [0, 1, 2, 3],  # Custom group labels\n",
    "    default=4  # Catch-all for invalid/missing cases\n",
    ")\n",
    "\n",
    "#checking if all the organization fall into valud groups (not 3)\n",
    "orgs_in_groups = (\n",
    "    df_eda\n",
    "    .groupby(['user_group'])\n",
    "    .agg(organization_count=('organization_id', 'nunique'))\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "orgs_in_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf096e4-89bc-4094-b78e-eb4f99129c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb004693-aef1-4410-88bb-19711e270488",
   "metadata": {},
   "source": [
    "#### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd64fa-052a-4a21-a667-70ddc170d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting custom color schema\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"steelblue\", \"white\", \"orange\"])\n",
    "custom_cmap2 = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"lightsteelblue\", \"white\", \"orange\"])\n",
    "custom_cmap3 = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"white\", \"steelblue\"])\n",
    "custom_cmap4 = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"white\", \"orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130dec7-ac6f-4cd1-a99e-28eef662e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing convesion per days to converst since first action\n",
    "\n",
    "converters = (\n",
    "    df_eda[df_eda['converted'] == 1]\n",
    "    .groupby('organization_id')\n",
    "    .agg(days_to_convert=('days_to_convert_tr', 'min')) #change here days_to_convert_tr to days_to_convert_fa to check that parameter\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Group by days_to_convert\n",
    "time_to_convert = (\n",
    "    converters\n",
    "    .groupby('days_to_convert')\n",
    "    .agg(total_orgs=('organization_id', 'nunique'))\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add cumulative converters and % share\n",
    "time_to_convert['cumulative_converters'] = time_to_convert['total_orgs'].cumsum()\n",
    "total_converted = time_to_convert['total_orgs'].sum()\n",
    "time_to_convert['cumulative_percent'] = (time_to_convert['cumulative_converters'] / total_converted * 100).round(1)\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add absolute converters (left y-axis)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=time_to_convert['days_to_convert'],\n",
    "        y=time_to_convert['total_orgs'],\n",
    "        name='Daily Converters',\n",
    "        marker_color='steelblue',\n",
    "        yaxis='y1'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cumulative % converters (right y-axis)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=time_to_convert['days_to_convert'],\n",
    "        y=time_to_convert['cumulative_percent'],\n",
    "        name='Cumulative %',\n",
    "        mode='lines+markers',\n",
    "        marker_color='orange',\n",
    "        yaxis='y2'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout with dual axes\n",
    "fig.update_layout(\n",
    "    title='Daily and Cumulative Conversion by Days Since Trial Start',\n",
    "    xaxis=dict(title='Trial Day', dtick=1),\n",
    "    yaxis=dict(\n",
    "        title='Number of Converted Orgs',\n",
    "        side='left',\n",
    "        showgrid=False\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Cumulative % of Conversions',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        range=[0, 100]\n",
    "    ),\n",
    "    legend=dict(x=0.01, y=0.99),\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3b1ab-3acf-4f71-a77e-050c586d0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda['trial_start_week'] = df_eda['trial_start'].dt.to_period('W').apply(lambda r: r.start_time).dt.normalize()\n",
    "df_eda['conversion_week'] = df_eda['converted_at'].dt.to_period('W').apply(lambda r: r.start_time).dt.normalize()\n",
    "\n",
    "df_eda['weeks_since_trial_start'] = ((df_eda['conversion_week'] - df_eda['trial_start_week']).dt.days // 7).clip(lower=0)\n",
    "\n",
    "converted = df_eda[df_eda['converted'] == 1].copy()\n",
    "\n",
    "conversion_counts = (\n",
    "    converted.groupby(['trial_start_week', 'weeks_since_trial_start'])\n",
    "    .agg(converted_orgs=('organization_id', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cohort_sizes = (\n",
    "    df_eda.groupby('trial_start_week')\n",
    "    .agg(total_orgs=('organization_id', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "conversion_counts = conversion_counts.merge(cohort_sizes, on='trial_start_week')\n",
    "conversion_counts['conversion_rate'] = conversion_counts['converted_orgs'] / conversion_counts['total_orgs']\n",
    "\n",
    "heatmap_data = conversion_counts.pivot(index='trial_start_week', columns='weeks_since_trial_start', values='conversion_rate')\n",
    "\n",
    "heatmap_data = heatmap_data.sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "ax = sns.heatmap(heatmap_data, annot=True, fmt=\".1%\", cmap=custom_cmap, cbar_kws={'label': 'Conversion Rate'})\n",
    "\n",
    "ax.set_yticklabels([d.strftime('%Y-%m-%d') for d in heatmap_data.index], rotation=0)\n",
    "\n",
    "plt.title('Weekly Cohort Conversion Rate Heatmap')\n",
    "plt.ylabel('Trial Start Week')\n",
    "plt.xlabel('Weeks Since Trial Start')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b0c21-89bc-444f-86ef-5b17f49eea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[['ts_start', 'trial_start', 'trial_end', 'converted_at']].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f46ad1-6f85-4046-b5c7-3e15dd624f38",
   "metadata": {},
   "source": [
    "### Observtions: \n",
    "- All converted organizations bought the subscriotion at least 2 weeks after trial start and first action\n",
    "- Most of them did that within the last week of trial\n",
    "- Around a half of converters did it after the trial end (30 days). Of those, most converted by day 45 since trial start.\n",
    "- Some weeks were better than the others in terms of overall performance\n",
    "\n",
    "#### Questions: \n",
    "- Does this mean that these organizations were consistently active during the first 2 weeks of trial?\n",
    "- Was this conversion pattern consistent across install cohorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3591634-1abd-4a90-bdbe-552fcf05495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_group_aliases = {\n",
    "    1: 'Converted during trial, active later (19 total)',\n",
    "    2: 'Converted during trial, inactive later (80 total)',\n",
    "    3: 'Converted after trial (107 total)'\n",
    "}\n",
    "\n",
    "user_groups = sorted(df_eda['user_group'].unique())\n",
    "\n",
    "for group in user_groups:\n",
    "    df_group = df_eda[(df_eda['converted'] == 1) & (df_eda['user_group'] == group)].copy()\n",
    "\n",
    "    # Heatmap 1: active_weeks vs converted_week\n",
    "    heatmap_data_1 = df_group.pivot_table(\n",
    "        index='converted_week',\n",
    "        columns='active_weeks',\n",
    "        values='organization_id',\n",
    "        aggfunc=pd.Series.nunique,\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Heatmap 2: active_weeks vs max_active_week\n",
    "    heatmap_data_2 = df_group.pivot_table(\n",
    "        index='converted_week',\n",
    "        columns='max_active_week',\n",
    "        values='organization_id',\n",
    "        aggfunc=pd.Series.nunique,\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Skip if both are empty or zero-sum\n",
    "    if (heatmap_data_1.empty or heatmap_data_1.values.sum() == 0) and \\\n",
    "       (heatmap_data_2.empty or heatmap_data_2.values.sum() == 0):\n",
    "        print(f\"Skipping group '{group}' â€” no data to plot.\")\n",
    "        continue\n",
    "\n",
    "    # Normalize for annotations\n",
    "    heatmap_norm_1 = heatmap_data_1 / heatmap_data_1.values.sum() if heatmap_data_1.values.sum() > 0 else heatmap_data_1\n",
    "    heatmap_norm_2 = heatmap_data_2 / heatmap_data_2.values.sum() if heatmap_data_2.values.sum() > 0 else heatmap_data_2\n",
    "    \n",
    "    annot_1 = heatmap_norm_1.applymap(lambda x: f\"{x:.1%}\" if x > 0 else \"\")\n",
    "    annot_2 = heatmap_norm_2.applymap(lambda x: f\"{x:.1%}\" if x > 0 else \"\")\n",
    "\n",
    "    alias = user_group_aliases.get(group, str(group))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 3), sharey=True)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        heatmap_data_1,\n",
    "        annot=annot_1,\n",
    "        fmt='',\n",
    "        cmap=custom_cmap,\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        cbar_kws={'label': ''},\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f'Converted Week vs Active Weeks\\n {alias}')\n",
    "    axes[0].set_xlabel('Active Weeks')\n",
    "    axes[0].set_ylabel('Converted Week')\n",
    "    axes[0].invert_yaxis()\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap_data_2,\n",
    "        annot=annot_2,\n",
    "        fmt='',\n",
    "        cmap=custom_cmap,\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        cbar_kws={'label': 'Unique Organizations'},\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(f'Convertred Week vs Max Active Week\\n {alias}')\n",
    "    axes[1].set_xlabel('Max Active Week')\n",
    "    axes[1].set_ylabel('')  # Shared y-axis, so leave blank\n",
    "    axes[1].invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2eb82-c23b-4d51-b371-7d891ee04b42",
   "metadata": {},
   "source": [
    "#### Observtions & Questions: \n",
    "- Absolute majority of converters (group 2 and 3) was only active on week 1 (0-6 days since trial start), but convert around the time of trial end.\n",
    "- What is weekly retention like for convertes and non-converters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5048b63-f629-4ba7-8b30-01e101609fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = df_eda.copy()\n",
    "\n",
    "# Precompute total unique orgs per conversion group\n",
    "total_orgs_per_group = df_viz.groupby('user_group')['organization_id'].nunique()\n",
    "\n",
    "# Step 1: DAILY retention\n",
    "daily_counts = (\n",
    "    df_viz\n",
    "    .groupby(['active_days', 'user_group'])['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='active_orgs')\n",
    ")\n",
    "\n",
    "# Normalize\n",
    "daily_counts['retention'] = (\n",
    "    daily_counts['active_orgs'] / \n",
    "    daily_counts['user_group'].map(total_orgs_per_group)\n",
    ")\n",
    "daily_counts = daily_counts.rename(columns={'active_days': 'period'})\n",
    "\n",
    "# Step 2: WEEKLY retention\n",
    "weekly_counts = (\n",
    "    df_viz\n",
    "    .groupby(['active_weeks', 'user_group'])['organization_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='active_orgs')\n",
    ")\n",
    "\n",
    "weekly_counts['retention'] = (\n",
    "    weekly_counts['active_orgs'] /\n",
    "    weekly_counts['user_group'].map(total_orgs_per_group)\n",
    ")\n",
    "weekly_counts = weekly_counts.rename(columns={'active_weeks': 'period'})\n",
    "\n",
    "# Pivot for heatmap format\n",
    "daily_pivot = daily_counts.pivot(index='user_group', columns='period', values='retention')\n",
    "weekly_pivot = weekly_counts.pivot(index='user_group', columns='period', values='retention')\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "hm = sns.heatmap(\n",
    "    daily_pivot, \n",
    "    annot=True, fmt=\".0%\", cmap=custom_cmap3, ax=axes[0]\n",
    ")\n",
    "\n",
    "# Rotate annotation text by 90 degrees\n",
    "for text in hm.texts:\n",
    "    text.set_rotation(90)\n",
    "\n",
    "axes[0].set_xlabel(\"Distinct Days Active\")\n",
    "axes[0].set_ylabel(\"User Group\")\n",
    "\n",
    "sns.heatmap(\n",
    "    weekly_pivot, \n",
    "    annot=True, fmt=\".0%\", cmap=custom_cmap4, ax=axes[1]\n",
    ")\n",
    "axes[1].set_xlabel(\"Distinct Weeks Active\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Activity by Period and User Group\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1462d-5f64-4955-a4c3-230e705daf31",
   "metadata": {},
   "source": [
    "#### Observtions: \n",
    "- Organizations in **Group 1** were active on at least 3 distinct days and most of them were active on 3-5 distinct weeks.\n",
    "- 89% of organizations in **Group 2** were active on just one week\n",
    "- **Group 3** (converted after trial) and **Group 0** (not converted) show very similar daily and weekly engagement patterns. Therefore, these patterns are bad predictors for conversion, but engagement metrics of Group 3 could have the key to difference between converters and non-converters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308c4ee-f0a8-4861-b6f2-57503fe01902",
   "metadata": {},
   "source": [
    "### Engagement rates per activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35240394-cdfa-4589-808e-4359e5a868aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously I looked at all the data available, but for activity engagement I will look only at actions before conversion\n",
    "#df_cap = df_eda[df_eda['days_since_first_action'] < 14].reset_index(drop=True)\n",
    "df_cap = df_short.copy()\n",
    "df_cap = df_cap[df_cap['ts_start'] <= df_cap['converted_at']].reset_index(drop=True)\n",
    "df_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c955736-a936-4afc-abb7-bdc37fae3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding hepler columns\n",
    "df_cap_time = add_time_features(df_cap)\n",
    "#df_cap_time.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf304d-9144-4f31-9d18-c2a2d3028091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging user group from df_eda\n",
    "org_user_group = (\n",
    "    df_eda\n",
    "    .sort_values(['organization_id', 'user_group'])\n",
    "    .groupby('organization_id', as_index=False)['user_group']\n",
    "    .first()\n",
    ")\n",
    "\n",
    "df_cap_time = df_cap_time.merge(org_user_group, on='organization_id', how='left')\n",
    "df_cap_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102be55-7f39-4c56-b255-84393945b4ca",
   "metadata": {},
   "source": [
    "### Identifying activities with the highest engagement and lowest speed of discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d10af1-14e7-470b-922c-6288e0f7b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding acitivity name mapping\n",
    "\n",
    "mapping = {\n",
    "    \"ShiftDetails.View.Opened\": \"Other.Both\",\n",
    "    \"Scheduling.OpenShiftRequest.Created\": \"Other.Employee\",\n",
    "    \"Absence.Request.Created\": \"Other.Employee\",\n",
    "    \"Absence.Request.Approved\": \"Other.Admin\",\n",
    "    \"PunchClockEndNote.Add.Completed\": \"Other.Employee\",\n",
    "    \"PunchClockStartNote.Add.Completed\": \"Other.Employee\",\n",
    "    \"Absence.Request.Rejected\": \"Other.Admin\",\n",
    "    \"Scheduling.ShiftSwap.Created\": \"Other.Employee\",\n",
    "    \"Scheduling.ShiftHandover.Created\": \"Other.Employee\",\n",
    "    \"Timesheets.BulkApprove.Confirmed\": \"Other.Admin\",\n",
    "    \"Scheduling.ShiftHandover.Accepted\": \"Other.Both\",\n",
    "    \"PunchClock.PunchedOut\": \"Other.Employee\",\n",
    "    \"PunchClock.Entry.Edited\": \"Other.Admin\",\n",
    "    \"Integration.Xero.PayrollExport.Synced\": \"Other.Admin\",\n",
    "    \"Break.Activate.Started\": \"Other.Employee\",\n",
    "    \"Scheduling.ShiftSwap.Accepted\": \"Other.Both\",\n",
    "    \"Break.Activate.Finished\": \"Other.Employee\",\n",
    "    \"Revenue.Budgets.Created\": \"Other.Admin\",\n",
    "    \"Scheduling.OpenShiftRequest.Approved\": \"Other.Admin\",\n",
    "    \"Shift.View.Opened\": \"Other.Both\"\n",
    "}\n",
    "\n",
    "# Adding new column to df_cap_time\n",
    "df_cap_time['activity_name_short'] = df_cap_time['activity_name'].map(mapping).fillna(df_cap_time['activity_name'])\n",
    "df_cap_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d8f87-2cff-4041-ad37-7a8921c7012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_engagement_and_speed_styled(\n",
    "    df,\n",
    "    category_col='activity_name',\n",
    "    user_group_col='user_group',\n",
    "    org_col='organization_id',\n",
    "    time_col='ts_start',\n",
    "    days_min=None,\n",
    "    days_max=None,\n",
    "    baseline='org_first_activity',\n",
    "    custom_cmap=custom_cmap\n",
    "):\n",
    "\n",
    "    # --- Engagement part ---\n",
    "    if days_min is not None:\n",
    "        df = df[df['hours_since_first_action'] >= days_min]\n",
    "    if days_max is not None:\n",
    "        df = df[df['hours_since_first_action'] <= days_max]\n",
    "\n",
    "    engagement_summary = (\n",
    "        df.groupby([category_col, user_group_col])\n",
    "          .agg(orgs_engaged=(org_col, 'nunique'))\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    pivot_orgs_engaged = engagement_summary.pivot(\n",
    "        index=category_col,\n",
    "        columns=user_group_col,\n",
    "        values='orgs_engaged'\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    pivot_orgs_engaged.columns = pd.MultiIndex.from_product(\n",
    "        [['orgs_engaged'], pivot_orgs_engaged.columns]\n",
    "    )\n",
    "\n",
    "    total_orgs_per_group = df.groupby(user_group_col)[org_col].nunique().to_dict()\n",
    "\n",
    "    engagement_rate = pivot_orgs_engaged.copy()\n",
    "    groups = engagement_rate.columns.get_level_values(1).unique()\n",
    "\n",
    "    for group in groups:\n",
    "        engagement_rate[('engagement_rate', group)] = (\n",
    "            engagement_rate[('orgs_engaged', group)] / total_orgs_per_group.get(group, 1)\n",
    "        )\n",
    "\n",
    "    engagement_rate = engagement_rate.loc[:, engagement_rate.columns.get_level_values(0) == 'engagement_rate']\n",
    "\n",
    "    engagement_df = pd.concat([pivot_orgs_engaged, engagement_rate], axis=1)\n",
    "\n",
    "    # --- Discovery speed part ---\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    org_first_ts = df.groupby(org_col)[time_col].min().rename('org_first_ts')\n",
    "\n",
    "    org_activity_first_ts = (\n",
    "        df.groupby([org_col, user_group_col, category_col])[time_col]\n",
    "        .min()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    org_activity_first_ts = org_activity_first_ts.merge(org_first_ts, left_on=org_col, right_index=True)\n",
    "\n",
    "    if baseline == 'org_first_activity':\n",
    "        org_activity_first_ts['discovery_delay'] = (\n",
    "            org_activity_first_ts[time_col] - org_activity_first_ts['org_first_ts']\n",
    "        ).dt.total_seconds() / (24 * 3600)\n",
    "    else:\n",
    "        org_activity_first_ts['discovery_delay'] = (\n",
    "            org_activity_first_ts[time_col] - org_activity_first_ts[time_col].min()\n",
    "        ).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "    speed_summary = (\n",
    "        org_activity_first_ts\n",
    "        .groupby([user_group_col, category_col])['discovery_delay']\n",
    "        .agg(median_days='median', mean_days='mean')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    speed_melted = speed_summary.melt(\n",
    "        id_vars=[user_group_col, category_col],\n",
    "        value_vars=['median_days', 'mean_days'],\n",
    "        var_name='metric',\n",
    "        value_name='value'\n",
    "    )\n",
    "\n",
    "    speed_pivot = speed_melted.pivot_table(\n",
    "        index=category_col,\n",
    "        columns=['metric', user_group_col],\n",
    "        values='value'\n",
    "    ).round(2)\n",
    "\n",
    "    # --- New: Calculate average order of discovery per activity and user group ---\n",
    "    # Assign order per organization: sort activities by their first timestamp per org & user group\n",
    "    def assign_order(group):\n",
    "        group = group.sort_values(time_col)\n",
    "        group['order'] = range(1, len(group)+1)\n",
    "        return group\n",
    "\n",
    "    # Reset index after apply to avoid ambiguity in groupby\n",
    "    org_activity_order = org_activity_first_ts.groupby([org_col, user_group_col]).apply(assign_order).reset_index(drop=True)\n",
    "\n",
    "    avg_order_summary = (\n",
    "        org_activity_order\n",
    "        .groupby([user_group_col, category_col])['order']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    avg_order_pivot = avg_order_summary.pivot(\n",
    "        index=category_col,\n",
    "        columns=user_group_col,\n",
    "        values='order'\n",
    "    ).round(2)\n",
    "\n",
    "    avg_order_pivot.columns = pd.MultiIndex.from_product([['avg_order'], avg_order_pivot.columns])\n",
    "\n",
    "    # --- Combine all data ---\n",
    "    combined = pd.concat([engagement_df, speed_pivot, avg_order_pivot], axis=1)\n",
    "    total_orgs = combined.loc[:, combined.columns.get_level_values(0) == 'orgs_engaged'].sum(axis=1)\n",
    "    combined = combined.loc[total_orgs.sort_values(ascending=False).index]\n",
    "\n",
    "    # --- Filter by engagement rate >= 0.02 in any user group ---\n",
    "    engagement_rate_cols = [col for col in combined.columns if col[0] == 'engagement_rate']\n",
    "    if engagement_rate_cols:\n",
    "        combined = combined[combined[engagement_rate_cols].ge(0.02).any(axis=1)] # <==== set to something else\n",
    "\n",
    "    # --- Reorder columns ---\n",
    "    metrics_order = ['orgs_engaged', 'engagement_rate', 'median_days', 'mean_days', 'avg_order']\n",
    "    all_groups = sorted(combined.columns.get_level_values(1).unique())\n",
    "\n",
    "    new_col_order = []\n",
    "    for metric in metrics_order:\n",
    "        for group in all_groups:\n",
    "            if (metric, group) in combined.columns:\n",
    "                new_col_order.append((metric, group))\n",
    "\n",
    "    combined = combined[new_col_order]\n",
    "\n",
    "    # --- Styling ---\n",
    "    if custom_cmap is None:\n",
    "        custom_cmap = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "    format_dict = {\n",
    "        **{('engagement_rate', group): '{:.1%}' for group in all_groups if ('engagement_rate', group) in combined.columns},\n",
    "        **{('median_days', group): '{:.2f}' for group in all_groups if ('median_days', group) in combined.columns},\n",
    "        **{('mean_days', group): '{:.2f}' for group in all_groups if ('mean_days', group) in combined.columns},\n",
    "        **{('avg_order', group): '{:.2f}' for group in all_groups if ('avg_order', group) in combined.columns},\n",
    "    }\n",
    "\n",
    "    subset_columns = [('engagement_rate', group) for group in all_groups if ('engagement_rate', group) in combined.columns]\n",
    "    subset_columns += [('avg_order', group) for group in all_groups if ('avg_order', group) in combined.columns]\n",
    "    #subset_columns += [('mean_days', group) for group in all_groups if ('mean_days', group) in combined.columns]\n",
    "\n",
    "    styled_df = (\n",
    "        combined.style\n",
    "        .background_gradient(subset=subset_columns, cmap=custom_cmap) #axis=1 for per-row color scale\n",
    "        .format(format_dict)\n",
    "        .set_caption(\"Engagement rate, discovery speed, and average order of discovery metrics\")\n",
    "    )\n",
    "\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78420ad-12b8-4486-a272-e97d3328343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_engagement_and_speed_styled(\n",
    "    df,\n",
    "    category_col='activity_name',\n",
    "    user_group_col='user_group',\n",
    "    org_col='organization_id',\n",
    "    time_col='ts_start',\n",
    "    days_min=None,\n",
    "    days_max=None,\n",
    "    baseline='org_first_activity',\n",
    "    metric_col='records',\n",
    "    threshold=1,        \n",
    "    custom_cmap=custom_cmap\n",
    "):\n",
    "    # --- Filter date range ---\n",
    "    if days_min is not None:\n",
    "        df = df[df['days_since_first_action'] >= days_min]\n",
    "    if days_max is not None:\n",
    "        df = df[df['days_since_first_action'] <= days_max]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # --- Engagement part with threshold ---\n",
    "    # Count an org as engaged if sum(metric_col) >= threshold\n",
    "    org_metric_sum = (\n",
    "        df.groupby([category_col, user_group_col, org_col])[metric_col].sum().reset_index()\n",
    "    )\n",
    "    engaged_orgs = (\n",
    "        org_metric_sum[org_metric_sum[metric_col] >= threshold]\n",
    "        .groupby([category_col, user_group_col])[org_col]\n",
    "        .nunique()\n",
    "        .reset_index(name='orgs_engaged')\n",
    "    )\n",
    "\n",
    "    pivot_orgs_engaged = engaged_orgs.pivot(\n",
    "        index=category_col,\n",
    "        columns=user_group_col,\n",
    "        values='orgs_engaged'\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    pivot_orgs_engaged.columns = pd.MultiIndex.from_product(\n",
    "        [['orgs_engaged'], pivot_orgs_engaged.columns]\n",
    "    )\n",
    "\n",
    "    total_orgs_per_group = df.groupby(user_group_col)[org_col].nunique().to_dict()\n",
    "\n",
    "    engagement_rate = pivot_orgs_engaged.copy()\n",
    "    groups = engagement_rate.columns.get_level_values(1).unique()\n",
    "\n",
    "    for group in groups:\n",
    "        engagement_rate[('engagement_rate', group)] = (\n",
    "            engagement_rate[('orgs_engaged', group)] / total_orgs_per_group.get(group, 1)\n",
    "        )\n",
    "\n",
    "    engagement_rate = engagement_rate.loc[:, engagement_rate.columns.get_level_values(0) == 'engagement_rate']\n",
    "    engagement_df = pd.concat([pivot_orgs_engaged, engagement_rate], axis=1)\n",
    "\n",
    "    # --- Average totals and daily averages ---\n",
    "    total_metric_per_org = df.groupby([category_col, user_group_col, org_col])[metric_col].sum().reset_index()\n",
    "    avg_total = (\n",
    "        total_metric_per_org.groupby([category_col, user_group_col])[metric_col].mean().reset_index()\n",
    "    )\n",
    "    avg_total_pivot = avg_total.pivot(index=category_col, columns=user_group_col, values=metric_col).round(2)\n",
    "    avg_total_pivot.columns = pd.MultiIndex.from_product([[f'avg_total_{metric_col}'], avg_total_pivot.columns])\n",
    "\n",
    "    df['date_only'] = df[time_col].dt.date\n",
    "    days_per_org = df.groupby([category_col, user_group_col, org_col])['date_only'].nunique().reset_index(name='distinct_days')\n",
    "    metric_and_days = total_metric_per_org.merge(days_per_org, on=[category_col, user_group_col, org_col])\n",
    "    metric_and_days['avg_daily'] = metric_and_days[metric_col] / metric_and_days['distinct_days']\n",
    "    avg_daily = (\n",
    "        metric_and_days.groupby([category_col, user_group_col])['avg_daily'].mean().reset_index()\n",
    "    )\n",
    "    avg_daily_pivot = avg_daily.pivot(index=category_col, columns=user_group_col, values='avg_daily').round(2)\n",
    "    avg_daily_pivot.columns = pd.MultiIndex.from_product([[f'avg_daily_{metric_col}'], avg_daily_pivot.columns])\n",
    "\n",
    "    # --- Discovery speed ---\n",
    "    org_first_ts = df.groupby(org_col)[time_col].min().rename('org_first_ts')\n",
    "    org_activity_first_ts = (\n",
    "        df.groupby([org_col, user_group_col, category_col])[time_col].min().reset_index()\n",
    "    )\n",
    "    org_activity_first_ts = org_activity_first_ts.merge(org_first_ts, left_on=org_col, right_index=True)\n",
    "\n",
    "    if baseline == 'org_first_activity':\n",
    "        org_activity_first_ts['discovery_delay'] = (\n",
    "            org_activity_first_ts[time_col] - org_activity_first_ts['org_first_ts']\n",
    "        ).dt.total_seconds() / (24 * 3600)\n",
    "    else:\n",
    "        org_activity_first_ts['discovery_delay'] = (\n",
    "            org_activity_first_ts[time_col] - org_activity_first_ts[time_col].min()\n",
    "        ).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "    speed_summary = (\n",
    "        org_activity_first_ts.groupby([user_group_col, category_col])['discovery_delay']\n",
    "        .agg(median_days='median', mean_days='mean')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    speed_pivot = speed_summary.pivot(index=category_col, columns=user_group_col).round(2)\n",
    "\n",
    "    # --- Average order ---\n",
    "    def assign_order(group):\n",
    "        group = group.sort_values(time_col)\n",
    "        group['order'] = range(1, len(group) + 1)\n",
    "        return group\n",
    "    org_activity_order = org_activity_first_ts.groupby([org_col, user_group_col]).apply(assign_order).reset_index(drop=True)\n",
    "    avg_order_summary = (\n",
    "        org_activity_order.groupby([user_group_col, category_col])['order'].mean().reset_index()\n",
    "    )\n",
    "    avg_order_pivot = avg_order_summary.pivot(index=category_col, columns=user_group_col, values='order').round(2)\n",
    "    avg_order_pivot.columns = pd.MultiIndex.from_product([['avg_order'], avg_order_pivot.columns])\n",
    "\n",
    "    # --- Combine ---\n",
    "    combined = pd.concat([engagement_df, avg_total_pivot, avg_daily_pivot, speed_pivot, avg_order_pivot], axis=1)\n",
    "    total_orgs = combined.loc[:, combined.columns.get_level_values(0) == 'orgs_engaged'].sum(axis=1)\n",
    "    combined = combined.loc[total_orgs.sort_values(ascending=False).index]\n",
    "\n",
    "    # --- Engagement rate filter ---\n",
    "    engagement_rate_cols = [col for col in combined.columns if col[0] == 'engagement_rate']\n",
    "    if engagement_rate_cols:\n",
    "        combined = combined[combined[engagement_rate_cols].ge(0.02).any(axis=1)]\n",
    "\n",
    "    # --- Reorder ---\n",
    "    #metrics_order = ['orgs_engaged', 'engagement_rate', f'avg_total_{metric_col}', f'avg_daily_{metric_col}', 'median_days', 'mean_days', 'avg_order']\n",
    "    metrics_order = ['orgs_engaged', 'engagement_rate', f'avg_total_{metric_col}', f'avg_daily_{metric_col}', 'avg_order']\n",
    "    all_groups = sorted(combined.columns.get_level_values(1).unique())\n",
    "    new_col_order = [(m, g) for m in metrics_order for g in all_groups if (m, g) in combined.columns]\n",
    "    combined = combined[new_col_order]\n",
    "\n",
    "    # --- Style ---\n",
    "    if custom_cmap is None:\n",
    "        custom_cmap = sns.light_palette(\"green\", as_cmap=True)\n",
    "    format_dict = {\n",
    "        **{('engagement_rate', g): '{:.1%}' for g in all_groups if ('engagement_rate', g) in combined.columns},\n",
    "        **{(f'avg_total_{metric_col}', g): '{:.2f}' for g in all_groups if (f'avg_total_{metric_col}', g) in combined.columns},\n",
    "        **{(f'avg_daily_{metric_col}', g): '{:.2f}' for g in all_groups if (f'avg_daily_{metric_col}', g) in combined.columns},\n",
    "        **{('median_days', g): '{:.2f}' for g in all_groups if ('median_days', g) in combined.columns},\n",
    "        **{('mean_days', g): '{:.2f}' for g in all_groups if ('mean_days', g) in combined.columns},\n",
    "        **{('avg_order', g): '{:.2f}' for g in all_groups if ('avg_order', g) in combined.columns},\n",
    "    }\n",
    "    subset_columns = [('engagement_rate', g) for g in all_groups if ('engagement_rate', g) in combined.columns]\n",
    "    #subset_columns += [('avg_order', g) for g in all_groups if ('avg_order', g) in combined.columns]\n",
    "\n",
    "    return (\n",
    "        combined.style\n",
    "        .background_gradient(subset=subset_columns, cmap=custom_cmap)\n",
    "        .format(format_dict)\n",
    "        .set_caption(f\"Engagement rate, discovery speed, and average order of discovery metrics ({metric_col})\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4348e66-c644-46cf-b43e-e250f5e95392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the first 2 weeks of engagement\n",
    "# days_min and days_max are hours\n",
    "\n",
    "engagement = combined_engagement_and_speed_styled(\n",
    "    df_cap_time, \n",
    "    user_group_col='user_group', \n",
    "    metric_col='events', \n",
    "    category_col='activity_name_ext', \n",
    "    threshold=2,\n",
    "    days_min = 0, \n",
    "    days_max=0)\n",
    "\n",
    "engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51586868-981f-49b8-8d23-f50fea6cd452",
   "metadata": {},
   "source": [
    "##### Group description \n",
    "\n",
    "| Group   | Description                               | Total organizations |\n",
    "|---------|-------------------------------------------|----------------------|\n",
    "| Group 0 | Not converted                             | 760                  |\n",
    "| Group 1 | Converted during trial, active after      | 19                   |\n",
    "| Group 2 | Converted during trial, active after      | 80                   |\n",
    "| Group 3 | Converted after trial                     | 107                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9c94a-ca35-4f9a-8c5e-ec85668fd76c",
   "metadata": {},
   "source": [
    "### Observations \n",
    "#### Engagement\n",
    "- **Scheduling.Shift.Created**: core admin feature, but only used by **~90%** of organizations.\n",
    "- **Mobile.Schedule.Loaded**: core employee feature, used by **94.7% in Group 1**, and by only **40% in other** groups. This could mean that organizations that are likely to convert already run active businesses and have employees that check the schedules. Could also mean that these are big businesses.\n",
    "- __Group 1:__  It is the smallest group, so I will only consider the activities with 50%+ engagement. This group has the highest engagement with these features:\n",
    "    - Scheduling.Shift.AssignmentChanged\n",
    "    - PunchClock.PunchedIn\n",
    "    - Scheduling.Shift.Approved\n",
    "- __Groups 0 and 3:__  these groups had very similar activity patterns, which could be explained by the fact that both didn't convert during trial. But Group 3 had **twice higher** engagement with **Scheduling.Template.ApplyModal.Applied** which could be one of the reasons why they eventually converted.\n",
    "\n",
    "#### Unlock order\n",
    "- __Group 1__ has a more or less defined order of discovering activities that follows the engagegment rates: Scheduling.Shift.Created -> Mobile.Schedule.Loaded -> Scheduling.Shift.AssignmentChanged -> Communication.Message.Created -> PunchClock.PunchedIn\n",
    "- __Group 2__ has the least defined unlock order with most actions being between 1st and 3rd    \n",
    "- __Groups 0 and 3:__  again show very similar unlock patterns which might make it hard to predict conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479b09b-46d3-4797-a0dd-b5c5d5367271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engagement_summary_activity_filter(\n",
    "    df,\n",
    "    activities,\n",
    "    filter_type='exclude',      # 'exclude' or 'include'\n",
    "    match_mode='or',            # 'or' = any match, 'and' = all match\n",
    "    category_col='activity_name',\n",
    "    user_group_col='user_group',\n",
    "    org_col='organization_id',\n",
    "    time_col='ts_start',\n",
    "    days_min=None,\n",
    "    days_max=None,\n",
    "    baseline='org_first_activity',\n",
    "    custom_cmap=custom_cmap\n",
    "):\n",
    "\n",
    "    # --- Step 1: Identify activities per org ---\n",
    "    org_activities = df.groupby(org_col)[category_col].apply(set)\n",
    "\n",
    "    if match_mode == 'or':\n",
    "        matching_orgs = org_activities[\n",
    "            org_activities.apply(lambda acts: any(act in acts for act in activities))\n",
    "        ].index\n",
    "    elif match_mode == 'and':\n",
    "        matching_orgs = org_activities[\n",
    "            org_activities.apply(lambda acts: all(act in acts for act in activities))\n",
    "        ].index\n",
    "    else:\n",
    "        raise ValueError(\"match_mode must be either 'or' or 'and'.\")\n",
    "\n",
    "    # --- Step 2: Apply filter_type ---\n",
    "    if filter_type == 'exclude':\n",
    "        df_filtered = df[~df[org_col].isin(matching_orgs)].copy()\n",
    "    elif filter_type == 'include':\n",
    "        df_filtered = df[df[org_col].isin(matching_orgs)].copy()\n",
    "    else:\n",
    "        raise ValueError(\"filter_type must be either 'exclude' or 'include'.\")\n",
    "\n",
    "    # --- Step 3: Reset index to avoid ambiguity in later groupby ---\n",
    "    df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "    # --- Step 4: Run main calculation/styling ---\n",
    "    return combined_engagement_and_speed_styled(\n",
    "        df_filtered,\n",
    "        category_col=category_col,\n",
    "        user_group_col=user_group_col,\n",
    "        org_col=org_col,\n",
    "        time_col=time_col,\n",
    "        days_min=days_min,\n",
    "        days_max=days_max,\n",
    "        baseline=baseline,\n",
    "        custom_cmap=custom_cmap\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35768f-c6ae-4060-a58d-293ff6299342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of activities to exclude\n",
    "#employee_activities = [ 'Mobile.Schedule.Loaded', 'PunchClock.PunchedIn', 'Absence.Request.Created', 'Scheduling.Availability.Set', 'Scheduling.OpenShiftRequest.Created']\n",
    "employee_activities = ['Mobile.Schedule.Loaded']\n",
    "\n",
    "no_employee = engagement_summary_activity_filter(\n",
    "    df_cap_time,\n",
    "    activities=employee_activities,\n",
    "    filter_type='exclude',\n",
    "    match_mode='or'\n",
    ")\n",
    "\n",
    "no_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fd747-4309-4342-bb44-1edbb1171b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shifts_views = engagement_summary_activity_filter(\n",
    "    df_cap_time,\n",
    "    activities=['Mobile.Schedule.Loaded', 'Scheduling.Shift.Created'],\n",
    "    filter_type='exclude',\n",
    "    match_mode='or'\n",
    ")\n",
    "\n",
    "no_shifts_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc9643-5c83-4c5b-bc11-d639997fe686",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_and_views = engagement_summary_activity_filter(\n",
    "    df_cap_time,\n",
    "    activities=['Mobile.Schedule.Loaded', 'Scheduling.Shift.Created'],\n",
    "    filter_type='include',\n",
    "    match_mode='and'\n",
    ")\n",
    "\n",
    "shifts_and_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2f84f-8477-4402-8e4a-c79897bf4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 50)\n",
    "#df_cap.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8072b-1434-4c82-8342-482a17f36323",
   "metadata": {},
   "source": [
    "#### Observations and assumptions: \n",
    "- The organizations that didn't engage in sheduling shifts, mainly engaged in viewing schedule on mobile and didn't engage in any meaningful activity involving scheduling or accounting. 4 converted organizations were engaged in Punching clock, but since there was no\n",
    "- Around 50% of total organizations (both converted and not) did not engage in any employee-driven activities. I assume that it is possible to use Planday in admin-only mode without explicit time tracking.\n",
    "- Almost all organizations (except 14) engaged with either Scheduling.Shift.Created or Mobile.Schedule.Loaded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed75e2-56d1-4157-b7cd-73aedb87e125",
   "metadata": {},
   "source": [
    "#### Engagement depth per user group\n",
    "- How many records per activity did they have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48073018-a44e-4717-9f19-867379bc5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "def records_kde_by_activity(\n",
    "    df,\n",
    "    activity_col='activity_name',\n",
    "    user_group_col='user_group',\n",
    "    org_col='organization_id',\n",
    "    records_col='records',\n",
    "    ts_col='ts_start',\n",
    "    min_days=None,\n",
    "    max_days=None,\n",
    "    plots_per_row=4,\n",
    "    plot=True\n",
    "):\n",
    "    # Prefilter dataset by days_since_first_action if provided\n",
    "    if min_days is not None:\n",
    "        df = df[df['days_since_first_action'] >= min_days]\n",
    "    if max_days is not None:\n",
    "        df = df[df['days_since_first_action'] <= max_days]\n",
    "\n",
    "    # Calculate days_active per org: number of unique days (date part of ts_col)\n",
    "    df['activity_date'] = pd.to_datetime(df[ts_col]).dt.date\n",
    "    days_active = df.groupby(org_col)['activity_date'].nunique().rename('days_active')\n",
    "    \n",
    "    # Aggregate total records per org per activity & user group\n",
    "    agg_df = (\n",
    "        df.groupby([activity_col, user_group_col, org_col])[records_col]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .merge(days_active.reset_index(), on=org_col, how='left')\n",
    "    )\n",
    "\n",
    "    # Calculate avg_daily records per org\n",
    "    agg_df['avg_daily'] = agg_df[records_col] / agg_df['days_active']\n",
    "    \n",
    "    # Apply log transform (add 1 to avoid log(0))\n",
    "    agg_df['log_total'] = np.log1p(agg_df[records_col])\n",
    "    agg_df['log_avg_daily'] = np.log1p(agg_df['avg_daily'])\n",
    "\n",
    "    if not plot:\n",
    "        return agg_df\n",
    "\n",
    "    activities = sorted(agg_df[activity_col].unique())\n",
    "\n",
    "    # We'll collect only plots that have data\n",
    "    plots_info = []  # tuples: (activity, 'log_total' or 'log_avg_daily', data_by_group)\n",
    "\n",
    "    for activity in activities:\n",
    "        subset = agg_df[agg_df[activity_col] == activity]\n",
    "        for metric in ['log_avg_daily']:  # ['log_total', 'log_avg_daily']\n",
    "            \n",
    "            # Check if there is at least one group with >1 data point\n",
    "            has_data = False\n",
    "            for group in sorted(subset[user_group_col].unique()):\n",
    "                group_data = subset[subset[user_group_col] == group][metric]\n",
    "                if len(group_data) > 1:\n",
    "                    has_data = True\n",
    "                    break\n",
    "            if has_data:\n",
    "                plots_info.append((activity, metric, subset))\n",
    "\n",
    "    total_plots = len(plots_info)\n",
    "    if total_plots == 0:\n",
    "        print(\"No valid data to plot.\")\n",
    "        return agg_df\n",
    "\n",
    "    rows = math.ceil(total_plots / plots_per_row)\n",
    "    fig, axes = plt.subplots(rows, plots_per_row, figsize=(5 * plots_per_row, 4 * rows))\n",
    "    if total_plots == 1:\n",
    "        axes = [axes] \n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "\n",
    "    for i, (activity, metric, subset) in enumerate(plots_info):\n",
    "        ax = axes[i]\n",
    "        for group in sorted(subset[user_group_col].unique()):\n",
    "            group_data = subset[subset[user_group_col] == group][metric]\n",
    "            if len(group_data) > 1 and group_data.var() > 0:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")  # suppress KDE warnings\n",
    "                    sns.kdeplot(group_data, label=str(group), fill=True, alpha=0.4, ax=ax)\n",
    "        title_metric = \"Log Total Records\" if metric == 'log_total' else \"Log Avg Daily Records\"\n",
    "        ax.set_title(f\"{activity} ({title_metric})\", fontsize=11)\n",
    "        ax.set_xlabel(f\"{title_metric} per Org + 1\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        if handles:\n",
    "            ax.legend(title=user_group_col)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb56f04-9213-42fe-ab8d-a7e61a8c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_records = records_kde_by_activity(df_cap_time, activity_col='activity_name_short', plots_per_row=4, min_days=0, max_days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112af34-a030-40d6-8b9b-03581b3399d8",
   "metadata": {},
   "source": [
    "#### Observations and assumptions: \n",
    "- From the distribution of max daily Mobile.Schedule.Loaded activities per user_group we can see that **Group 1** has the most daily views, which could mean that these are larger organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a1e01-53ba-419d-abdb-763e956cb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = records_kde_by_activity(df_cap_time, plot=False, user_group_col='converted', activity_col='activity_name', plots_per_row=4, min_days=0, max_days=6)\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b2383-7d36-4d1a-8248-12c9846f236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap.dtypes\n",
    "#df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079f6dc-a452-4df3-8f1d-e61dc7c5ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table = df_agg.pivot_table(\n",
    "    index=['organization_id', 'converted'], \n",
    "    columns='activity_name', \n",
    "    values='log_avg_daily', \n",
    "    aggfunc='first'  # or 'mean' if multiple rows per org-activity\n",
    ").reset_index().fillna(0)\n",
    "\n",
    "# Convert avg_daily values to binary flags: 1 if > 0 else 0\n",
    "activity_cols = feature_table.columns.difference(['organization_id', 'converted'])\n",
    "#feature_table[activity_cols] = (feature_table[activity_cols] > 0).astype(int)\n",
    "\n",
    "corr_matrix = feature_table.corr()\n",
    "\n",
    "corr_with_converted = corr_matrix['converted'].drop('converted').sort_values(ascending = False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Barplot with horizontal bars, positive to right, negative to left\n",
    "sns.barplot(x=corr_with_converted.values, y=corr_with_converted.index, palette='viridis')\n",
    "\n",
    "plt.axvline(0, color='black', linewidth=1)  # vertical line at 0 for reference\n",
    "plt.title('Correlation of Week 1 Activity Engagement with \"Converted\"')\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599d6d4-5768-44f3-944d-e7f6731bbccb",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Correlation bertween of all the features with converted is very weak (less than 0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b062d-444e-42d3-b373-ed19a91a4435",
   "metadata": {},
   "source": [
    "### State Transition probabilities: last actions before conversion/churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b452239-5c04-47f6-98d2-ba063c21162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reverse_transition_heatmap(df, title):\n",
    "    # Get the previous state for each current state per organization\n",
    "    df['prev_state'] = df.groupby('organization_id')['activity_name'].shift(1)\n",
    "    \n",
    "    # Drop rows without a previous state (first state per org)\n",
    "    transitions = (\n",
    "        df.dropna(subset=['prev_state'])\n",
    "        .groupby(['activity_name', 'prev_state'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    \n",
    "    all_states = sorted(set(df['activity_name']).union(set(df['prev_state'].dropna())))\n",
    "    \n",
    "    # Pivot so rows = current state, columns = previous state\n",
    "    transition_matrix = transitions.pivot(index='activity_name', columns='prev_state', values='count').fillna(0)\n",
    "    transition_matrix = transition_matrix.reindex(index=all_states, columns=all_states, fill_value=0)\n",
    "    \n",
    "    # Normalize by current state (rows) to get P(prev_state | current_state)\n",
    "    transition_prob = transition_matrix.div(transition_matrix.sum(axis=1), axis=0).fillna(0)\n",
    "    activity_order = sorted(transition_prob.index)\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        transition_prob,\n",
    "        labels=dict(x=\"Previous Activity\", y=\"Current Activity\", color=\"Reverse Transition Probability\"),\n",
    "        x=transition_prob.columns,\n",
    "        y=transition_prob.index,\n",
    "        color_continuous_scale='Blues',\n",
    "        title=title,\n",
    "        aspect=\"auto\",\n",
    "        width=900,\n",
    "        height=900\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(yaxis_autorange='reversed')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c7599-966c-447d-80b1-15de6dd0b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cap.copy()\n",
    "\n",
    "admin_activities = ['Absence.Request.Approved', 'Absence.Request.Rejected', 'Integration.Xero.PayrollExport.Synced', 'Timesheets.BulkApprove.Confirmed',\n",
    "                    'PunchClock.Entry.Edited', 'Revenue.Budgets.Created', 'Scheduling.OpenShiftRequest.Approved',\n",
    "                    'Scheduling.Template.ApplyModal.Applied', 'Communication.Message.Created', 'Scheduling.Shift.Approved',\n",
    "                    'Scheduling.Shift.HandoverAccepted'\n",
    "                   ]\n",
    "\n",
    "df = df[df['activity_name'].isin(admin_activities)]\n",
    "\n",
    "\n",
    "# Define custom sorting priority\n",
    "activity_priority = {\n",
    "    'Scheduling.Template.ApplyModal.Applied': 0,\n",
    "    'Scheduling.Shift.Created': 1,\n",
    "    'Scheduling.Shift.AssignmentChanged': 2\n",
    "}\n",
    "\n",
    "df['activity_priority'] = df['activity_name'].map(activity_priority).fillna(999)\n",
    "df_sorted = df.sort_values(['organization_id', 'ts_start', 'activity_priority']).reset_index(drop=True)\n",
    "\n",
    "# Creating group_id for consecutive runs of same org/activity_name/bulk\n",
    "df_sorted['group_id'] = (\n",
    "    (df_sorted['organization_id'] != df_sorted['organization_id'].shift()) |\n",
    "    (df_sorted['activity_name'] != df_sorted['activity_name'].shift()) \n",
    ").cumsum()\n",
    "\n",
    "df_rollup = (\n",
    "    df_sorted\n",
    "    .groupby('group_id')\n",
    "    .agg(\n",
    "        organization_id=('organization_id', 'first'),\n",
    "        activity_name=('activity_name', 'first'),\n",
    "        ts_start=('ts_start', 'first'),\n",
    "        converted=('converted', 'first'),\n",
    "        converted_at=('converted_at', 'first')\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_rollup = df_rollup[['organization_id', 'activity_name', 'ts_start', 'converted', 'converted_at']]\n",
    "df_rollup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475888ca-f452-4b67-8fe9-3d585adc3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by converted status\n",
    "\n",
    "#activities_to_exclude = ['Scheduling.Shift.Created', 'Mobile.Schedule.Loaded', 'Scheduling.Shift.AssignmentChanged'] \n",
    "employee_activities = [ 'PunchClock.PunchedIn', 'PunchClock.PunchedOut', 'Absence.Request.Created', 'Scheduling.Availability.Set', \n",
    "                        'Scheduling.OpenShiftRequest.Created', 'Break.Activate.Finished', 'Break.Activate.Started',\n",
    "                        'PunchClockEndNote.Add.Completed',  'PunchClockStartNote.Add.Completed', 'Scheduling.ShiftSwap.Created'\n",
    "                      ]\n",
    "admin_activities = ['Absence.Request.Approved', 'Absence.Request.Rejected', 'Integration.Xero.PayrollExport.Synced', 'Timesheets.BulkApprove.Confirmed',\n",
    "                    'PunchClock.Entry.Edited', 'Revenue.Budgets.Created', 'Scheduling.OpenShiftRequest.Approved',\n",
    "                    'Scheduling.Template.ApplyModal.Applied', 'Communication.Message.Created', 'Scheduling.Shift.Approved',\n",
    "                    'Scheduling.Shift.HandoverAccepted'\n",
    "                   ]\n",
    "\n",
    "df = df_rollup.copy()\n",
    "#df = df[~df['activity_name'].isin(employee_activities)]\n",
    "#df = df[df['activity_name'].isin(admin_activities)]\n",
    "\n",
    "df_converted = df[df['converted'] == 1]\n",
    "df_not_converted = df[df['converted'] == 0]\n",
    "\n",
    "# Prepare chains for each group\n",
    "#df_chains_converted = prepare_chains(df_converted)\n",
    "#df_chains_not_converted = prepare_chains(df_not_converted)\n",
    "\n",
    "# Plot heatmaps\n",
    "#plot_reverse_transition_heatmap(df_chains_converted, \"Reverse Transition - Converted\")\n",
    "#plot_reverse_transition_heatmap(df_chains_not_converted, \"Reverse Transition - Not Converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821913c0-78f4-4998-8c24-117ef2763ffc",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Most employee-driven activities result in browsing mobile schedule which could mean that this is the default screen in the mobile app.\n",
    "- Since Admins are the ones making a decision abour conversion, I excluded employee-only activities and scheduling shifts from the list to get the most common conversion and churn drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b67a9f-b5db-4eb3-bf5d-228af4200d39",
   "metadata": {},
   "source": [
    "## Feature engineering and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3541f6-0c77-4157-89f7-f15bc3895ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 100)\n",
    "df_fe = df_cap_time.copy()\n",
    "df_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd4bab-350f-41ab-b83e-cd37f3eab66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim = (\n",
    "    df_fe\n",
    "    .groupby(['organization_id', 'converted'])\n",
    "    .agg(\n",
    "        days_to_action =('days_to_action', 'first'),\n",
    "        first_action_weekday=('first_action_weekday', 'first'),\n",
    "        first_action_hour=('first_action_hour', 'first'),\n",
    "        active_span_days =('active_span_days', 'first'),\n",
    "        active_days =('active_days', 'first'),\n",
    "        active_weeks =('active_weeks', 'first'),\n",
    "        activity_density =('activity_density', 'first')\n",
    "    )\n",
    "    .sort_values(['organization_id', 'converted'])\n",
    "    .reset_index()\n",
    ")\n",
    "#df_dim['first_action_week'] = df_fe['first_action_ts'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "d1_flag = df_fe[df_fe['days_since_first_action'] == 1] \\\n",
    "            .groupby('organization_id').size() \\\n",
    "            .to_frame('d1_eng')\n",
    "\n",
    "d1_flag['d1_eng'] = 1\n",
    "df_dim = df_dim.merge(d1_flag[['d1_eng']], on='organization_id', how='left')\n",
    "df_dim['d1_eng'] = df_dim['d1_eng'].fillna(0).astype(int)\n",
    "\n",
    "df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5d13f-310e-43f0-a079-aff804d11337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe_agg = records_kde_by_activity(df_fe, plot=False, user_group_col='converted', \n",
    "                                    activity_col='activity_name_short', plots_per_row=4, \n",
    "                                    min_days=0, max_days=31) #analysing the first 2 weeks\n",
    "df_fe_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d09fb-053d-4347-b3f6-65ae14c6fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on just 2 days of engagement\n",
    "metrics_df = df_fe_agg.pivot_table(\n",
    "        index='organization_id',\n",
    "        columns='activity_name_short',\n",
    "        values='avg_daily',\n",
    "        aggfunc='mean',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9746da-cbc1-4fe7-8acb-4242e8323df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_var_cols = metrics_df.columns[metrics_df.nunique() <= 1]\n",
    "print(\"Columns with zero or one unique value:\", zero_var_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c17c94-f076-4955-9b0c-cc4cf293bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_dim.merge(metrics_df, on='organization_id', how='left')\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40beb6-2d1d-4316-9f51-c02c29ea5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_feat.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2160aa6-a0af-4020-b002-6c6276aacf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_feat.head())\n",
    "print(df_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918a8a5-34af-4f84-8a8d-3978ff2d717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn imbalanced-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7ee6e-d8ee-41e1-bc36-2bc52038ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "df=df_feat.copy()\n",
    "\n",
    "X = df.drop(columns=['converted', 'organization_id'])\n",
    "y = df['converted']\n",
    "\n",
    "#categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "#X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)  \n",
    "\n",
    "# Now you can continue with scaling, modeling, etc.\n",
    "scaler = StandardScaler()\n",
    "#X_scaled = pd.DataFrame(scaler.fit_transform(X_encoded), columns=X_encoded.columns)\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train, predict, and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = None\n",
    "    # For AUROC, need predicted probabilities for the positive class\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):  # fallback\n",
    "        y_proba = model.decision_function(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    # AUROC\n",
    "    if y_proba is not None:\n",
    "        auroc = roc_auc_score(y_test, y_proba)\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "    else:\n",
    "        print(\"AUROC: N/A (no probability estimates available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f58512-6f49-48c0-8bfc-e1b962e84598",
   "metadata": {},
   "source": [
    "#### Prediction result\n",
    "\n",
    "XGBoost model has shown the best accuracy overall. Although more feature engineering and pruning to improve Converters prediction, we can look at the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab6061-ffb3-4aa9-a373-e103610c8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Create SHAP explainer for tree-based models\n",
    "explainer = shap.Explainer(xgb_model, X_train_res)\n",
    "\n",
    "# Calculate SHAP values for the training set\n",
    "shap_values = explainer(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10516899-3aa0-40a2-aacd-bfb34913502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_res, max_display=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633be0b9-8cf1-4918-ac45-a3133fb49cd7",
   "metadata": {},
   "source": [
    "#### Features\n",
    "The model was trained on the first 2 weeks of trial. According to the model, these are most important features that had a clear positive or negative contribution:\n",
    "\n",
    "Independent metrics:\n",
    "\n",
    "- _Active span days_: organizations with bigger active span are less likely to convert.\n",
    "- _Activity density_: organizations with higher activity density (ratio of active days to active weeks) are more likely to convert.\n",
    "\n",
    "Note: daily metrics were capped (only first 3 days), the above are historic\n",
    "\n",
    "App metrics:\n",
    "\n",
    "- Organizations with high _Shift assignement change rate_ are less likely to convert\n",
    "- Organizations with higher _Punch in to View_ ratio are more likely to convert\n",
    "- Organizations with higher _Template usage_ are more likely to convert\n",
    "- Organizations with low ratio of _Views to Shifts_ scheduled are more likely to convert -> this could indicate that actively run a business and already have real employees\n",
    "- Organizations with high number of _Messages Created_ scheduled are less likely to convert -> this could indicate that the messaging tool is not convenient, or that users don't understaand how to use other fun\n",
    "\n",
    "Punch in to punch out rate is quite low and often 0, so I assume that this is an optional action and employees are punched out automatically when their shift ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ef781-dc25-4b6c-9cad-99467327f5ad",
   "metadata": {},
   "source": [
    "### Trial goals\n",
    "\n",
    "Based on the model and EDA results, I am going to monitor these trial goals:\n",
    "\n",
    "Core functionality:\n",
    "Scheduling.Shift.Created -> Mobile.Schedule.Loaded -> Scheduling.Shift.Approved\n",
    "\n",
    "Admin functionality:\n",
    "Scheduling.Template.ApplyModal.Applied -> Scheduling.Shift.Created ->  Scheduling.Shift.AssignmentChanged -> Scheduling.Shift.Approved\n",
    "\n",
    "Since there is no hard sequence of events, I ordered them in the order or the most intuitive discovery in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b0986-7ba9-43d7-8709-a0028922b133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
